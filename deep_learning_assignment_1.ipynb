{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression implementation.\n",
    "The implementation of logistic regression here is as described in the lecture notes. The weights are created as a vector, each uniformly distributed on $[0,1]$. The 'back propogation' step takes place in the stochastic_gradient_descent function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_inputs, sgd_thresh = 10e-8, sgd_max_iters = 2500, alpha = 0.0001):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.weights = np.random.rand(self.n_inputs, 1).astype('f')\n",
    "        #print(self.weights)\n",
    "        self.bias = 1.0\n",
    "        self.alpha = alpha\n",
    "        self.sgd_thresh = sgd_thresh\n",
    "        self.sgd_max_iters = sgd_max_iters\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.array(x).reshape(-1,1)\n",
    "        return int(np.round(self.sigmoid(np.dot(self.weights.T, x) + self.bias)))\n",
    "    \n",
    "    def log_loss(self, X, Y):\n",
    "        #dot = [np.dot(self.weights.T, x) for x in X]\n",
    "        \n",
    "        y_hats = [self.sigmoid(np.dot(self.weights.T, x) + self.bias) for x in X]\n",
    "        self.yhats = y_hats\n",
    "        #print(np.sum(y_hats))\n",
    "        J_w_b = (-1/len(Y))*np.sum([y*np.log(y_hat)+(1-y)*np.log(1-y_hat) for y,y_hat in zip(Y,y_hats)])\n",
    "        return J_w_b\n",
    "\n",
    "    def stochastic_gradient_descent(self,X,Y):\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        J = self.log_loss(X,Y)\n",
    "        for i in range(self.sgd_max_iters):\n",
    "            idx = np.random.randint(len(X))\n",
    "            x_sample, y_sample = X[idx], Y[idx]\n",
    "            diff = self.predict(x_sample) - y_sample\n",
    "\n",
    "            self.weights = self.weights - (self.alpha*diff*x_sample).reshape(-1,1)\n",
    "\n",
    "            self.bias = self.bias - self.alpha*diff\n",
    "\n",
    "            \n",
    "            if self.log_loss(X,Y) - J < self.sgd_thresh:\n",
    "                break\n",
    "            #print(self.log_loss(X,Y), J)\n",
    "            J = self.log_loss(X,Y)\n",
    "            \n",
    "    def score(self, x_test, y_test):\n",
    "        x_test = np.array(x_test)\n",
    "        y_test = np.array(y_test)\n",
    "        correct = 0\n",
    "        total = len(x_test)\n",
    "        for x,y in zip(x_test, y_test):\n",
    "            if self.predict(x) ==  y:\n",
    "                correct += 1\n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the moons400 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded in using pandas and is split into training and test sets using the train_test_split function from the model_selection module in the sci-kit learn package. On the harder of the two problems it achieves an accuracy of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/oisin/MAI_work/ongoing_assignments/DeepLearning/data/\"\n",
    "\n",
    "data = pd.read_csv(path+\"moons400.csv\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class']) \n",
    "\n",
    "lr = LogisticRegression(x_train.shape[1],sgd_max_iters=200,sgd_thresh=-np.inf,alpha=10e-2)\n",
    "lr.stochastic_gradient_descent(x_train,y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the blobs250 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe then on the easiest of all the data sets that logistic regression perfectly splits the datasets and scores 100% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path+\"blobs250.csv\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class']) \n",
    "\n",
    "lr = LogisticRegression(x_train.shape[1],sgd_max_iters=200,sgd_thresh=-np.inf,alpha=10e-2)\n",
    "lr.stochastic_gradient_descent(x_train,y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network Performance\n",
    "Below is the code for the implementation of the neural network. Since one of my enhancements was to implement deep neural networks (support for multiple layers), the code is identical to the enhancement code. This is simply because there was no point implementing code specific to one hidden layer and then implementing code for many layers when just generalising first does the trick. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Class\n",
    "This implementation has a Layer class, where the weights and layer activation function are stored. The weights are initially $\\mathcal{N}(0,1)$ as are the biases. The class implements \"fwdprop_ouput\" which calculates a and z for the Layer, as seen in the notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Class\n",
    "The Network class creates the layers, as directed by the inputs. These inputs are in the form of 2 lists, m and n say, where the $i^{th}$ member of m is the number of nodes in layer $i$, and the $i^{th}$ member of n is the activation function class for that layer (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function Class\n",
    "The activation functions are created with an \"f\" method and a \"deriv\" method. These represent the function itself and the derivative of the function, to be used in the back propagation step, as required by the chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, prev_no_nodes, no_nodes, activation_function, is_input=False):\n",
    "        if not(is_input):\n",
    "            self.weights = np.random.normal(0.01,1,size = (prev_no_nodes,no_nodes))\n",
    "            self.bias = np.random.normal(0.01,1, size = (no_nodes,1))\n",
    "            self.act_f = activation_function()\n",
    "            \n",
    "        else:\n",
    "            self.act_f = activation_function()\n",
    "        self.no_nodes = no_nodes\n",
    "        self.is_input = is_input\n",
    "\n",
    "    def fwdprop_output(self, X):\n",
    "        if self.is_input:\n",
    "            self.a = X\n",
    "            self.z = self.a\n",
    "            return X\n",
    "        X = X.reshape(-1,1)\n",
    "        self.z = np.dot(self.weights.T,X) + self.bias\n",
    "        self.a = self.act_f.f(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    \n",
    "    \n",
    "class IdentityActivation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def f(self,x):\n",
    "        return x\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ReluActivation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def f(self, x):\n",
    "        return np.maximum(x,0)\n",
    "        \n",
    "    def deriv(self, x):\n",
    "        x[x>0] = 1\n",
    "        x[x<0] = 0\n",
    "        return x\n",
    "        \n",
    "class SigmoidActivation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def f(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return self.f(x)*(1-self.f(x))\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, no_nodes_layer, activation_function, loss_function = \"log_loss\", lamb = 0):\n",
    "        '''\n",
    "        TODO: Sort out the fact that the first layer doesn't have an activation function (DOESNT FUCKIN NEED ONE AHAHHA)\n",
    "        TODO: Add the loss_functions\n",
    "        TODO: Finish train and predict/score\n",
    "        '''\n",
    "        self.no_nodes_layer = no_nodes_layer\n",
    "        self.activation_function = activation_function\n",
    "        self.input_size = no_nodes_layer[0]\n",
    "        self.no_layers = len(self.no_nodes_layer)\n",
    "        self.lamb = lamb\n",
    "        self.no_params = sum([self.no_nodes_layer[i]*self.no_nodes_layer[i-1] for i in range(1,len(self.no_nodes_layer))])\n",
    "        \n",
    "        if isinstance(no_nodes_layer, list):\n",
    "            '''\n",
    "            TODO: add functionality so that differnet layers can have different activation functions \n",
    "            '''\n",
    "            assert(self.no_nodes_layer[-1] == 1 or self.no_nodes_layer[-1] == 2)\n",
    "            self.layers = [Layer(self.no_nodes_layer[i-1],self.no_nodes_layer[i],activation_function[i]) for i in range(1,len(self.no_nodes_layer))]\n",
    "            input_layer = Layer(0,no_nodes=self.no_nodes_layer[0],activation_function=activation_function[0], is_input=True)\n",
    "            self.layers = [input_layer] + self.layers\n",
    "\n",
    "        else:\n",
    "            #Come up with a better default\n",
    "            self.layers = [Layer(no_nodes_layer,1)]\n",
    "\n",
    "        self.W = np.zeros(0)\n",
    "        #turn all the weight matrices into one long weight vector so one can find the l_p norm of it.\n",
    "        self.W = np.concatenate([self.W] +[layer.weights.flatten() for layer in self.layers[1:]])\n",
    "        #print(np.linalg.norm(self.W, 2))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def log_loss(self,X,Y):\n",
    "        y_hats = [self.fwdpropagate(x) for x in X]\n",
    "        J_w_b = (-1)*sum([y*np.log(y_hat)+(1-y)*np.log(1-y_hat) for y,y_hat in zip(Y,y_hats)])\n",
    "        J_w_b += self.lamb*np.linalg.norm(self.W, 2)\n",
    "        return J_w_b\n",
    "        \n",
    "        \n",
    "    def fwdpropagate(self, _input):\n",
    "        \n",
    "        if len(_input) != self.layers[0].no_nodes:\n",
    "            print(f\"Input must be of length {self.layers[0].no_nodes}, it is of length {len(_input)}\")\n",
    "        \n",
    "        self.a_s = []   \n",
    "        a = _input\n",
    "\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            a = layer.fwdprop_output(a)\n",
    "            self.a_s.append(a)\n",
    "        \n",
    "        #self.layer_outputs = []\n",
    "        y_hat = a\n",
    "        #self.layer_outputs.append(y_hat)\n",
    "        \n",
    "        self.prediction = int(np.round(y_hat))\n",
    "\n",
    "        \n",
    "    def d_dout_sig(self, x):\n",
    "        return self.sigmoid(x)*(1.-self.sigmoid(x))\n",
    "    \n",
    "    \n",
    "    def d_dout_loss(self,X):\n",
    "        pass\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        self.fwdpropagate(x_test)\n",
    "        return self.prediction\n",
    "\n",
    "    def stochastic_gradient_descent(self, x_train, y_train, alpha = 0.001, n_iter = 10):\n",
    "        '''\n",
    "        Implementation of stochastic gradient descent with regularization. \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        \n",
    "        #combine the x and ys into one array to make suffling more straightforward\n",
    "        comb = np.c_[x_train.reshape(len(x_train), -1), y_train.reshape(len(y_train), -1)]\n",
    "        \n",
    "        #keep track of total training time and print initial accuracy sans training.\n",
    "        start = time.time()\n",
    "        ita = score = self.score(x_train,y_train)\n",
    "        print(f\"Initial training accuracy: {ita:.4f}%\")\n",
    "        for epoch in range(1,n_iter+1):\n",
    "            print(f\"Running epoch {epoch} of {n_iter}\")\n",
    "            \n",
    "            #This is just a fancy way of shuffling the x and ys together.\n",
    "            x_train_c = comb[:, :x_train.size//len(x_train)].reshape(x_train.shape)\n",
    "            y_train_c = comb[:, x_train.size//len(x_train):].reshape(y_train.shape)\n",
    "            np.random.shuffle(comb)\n",
    "\n",
    "        \n",
    "            for x,y in tqdm(zip(x_train_c, y_train_c)):\n",
    "                self.backward_propagate_error(x,y)\n",
    "                for i,layer in enumerate(self.layers):\n",
    "                    if layer.is_input:\n",
    "                        continue\n",
    "                    else:\n",
    "                        layer.weights -= (alpha*np.array(self.delta_w[i]))\n",
    "                        layer.bias -= alpha*np.array(self.delta_b[i])\n",
    "            \n",
    "            \n",
    "            score = self.score(x_train,y_train)\n",
    "            print(f\"Training accuracy: {score:.4f}%\")\n",
    "            #-----------------\n",
    "            \n",
    "            \n",
    "        end = time.time()\n",
    "        print(f\"Ran {n_iter} epochs in {end-start}\")\n",
    "    \n",
    "    \n",
    "    def score(self, x_test, y_test):\n",
    "        '''\n",
    "        Simple score function that gets the accuracy for a given test set\n",
    "        '''\n",
    "        x_test = np.array(x_test)\n",
    "        y_test = np.array(y_test)\n",
    "        correct = 0\n",
    "        total = len(x_test)\n",
    "        for x,y in zip(x_test, y_test):\n",
    "            self.fwdpropagate(x)\n",
    "            if self.predict(x) ==  y:\n",
    "                correct += 1\n",
    "        return correct/total\n",
    "    \n",
    "    \n",
    "    \n",
    "    def backward_propagate_error(self, x, y):\n",
    "        ''' The backward propogate error function is based on the algorithm from http://cs229.stanford.edu/notes2020spring/cs229-notes-deep_learning.pdf\n",
    "            (on the 2nd last page). Note the difference in derivatives of cost function from these notes, as \n",
    "            the notes use 1/2 squared error as the cost function.\n",
    "            \n",
    "            The function fisrt does the forward propagtaion to ensure the z's and a's of each layer exist. Then,\n",
    "            using the chain rule, we compute the gradients for each weight in each layer.\n",
    "        '''\n",
    "        self.fwdpropagate(x)\n",
    "        delta = [None]*self.no_layers  \n",
    "        delta_w = [None]*self.no_layers \n",
    "        delta_b = [None]*self.no_layers\n",
    "   \n",
    "        for i in range(self.no_layers - 1, -1, -1):\n",
    "            if i == self.no_layers - 1:\n",
    "                if y == 1:\n",
    "                    q = (y/(self.layers[i].a+10e6))\n",
    "                else:\n",
    "                    q = -1*((1-y)/(1-(self.layers[i].a-10e-6)))\n",
    "                delta[i] = -1*q*self.layers[i].act_f.deriv(self.layers[i].z)\n",
    "            else:\n",
    "                delta[i] = ((self.layers[i+1].weights)@(delta[i+1]))*self.layers[i].act_f.deriv(self.layers[i].z).reshape(-1,1)\n",
    "                delta_w[i+1] = ((delta[i+1])@(self.layers[i].a.reshape(1,-1))).T + 2*self.lamb*self.layers[i+1].weights\n",
    "                delta_b[i+1] = delta[i+1]\n",
    "\n",
    "        self.delta_w = delta_w\n",
    "        self.delta_b = delta_b\n",
    "        self.delta = delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, prev_no_nodes, no_nodes, activation_function, is_input=False):\n",
    "        if not(is_input):\n",
    "            self.weights = np.random.normal(0.0,1,size = (prev_no_nodes,no_nodes))\n",
    "            self.bias = np.random.normal(0.0,1, size = (no_nodes,1))\n",
    "            self.act_f = activation_function\n",
    "            \n",
    "        self.no_nodes = no_nodes\n",
    "        self.is_input = is_input\n",
    "\n",
    "    def fwdprop_output(self, X):\n",
    "        if self.is_input:\n",
    "            self.a = X\n",
    "            self.z = self.a\n",
    "            return X\n",
    "        X = X.reshape(-1,1)\n",
    "        self.z = np.dot(self.weights.T,X) + self.bias\n",
    "        self.a = self.act_f(self.z)\n",
    "        return self.a\n",
    "\n",
    "    \n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, no_nodes_layer, activation_function, loss_function = \"log_loss\", lamb = 0):\n",
    "        '''\n",
    "        TODO: Sort out the fact that the first layer doesn't have an activation function (DOESNT FUCKIN NEED ONE AHAHHA)\n",
    "        TODO: Add the loss_functions\n",
    "        TODO: Finish train and predict/score\n",
    "        '''\n",
    "        self.no_nodes_layer = no_nodes_layer\n",
    "        self.activation_function = activation_function\n",
    "        self.input_size = no_nodes_layer[0]\n",
    "        self.no_layers = len(self.no_nodes_layer)\n",
    "        self.lamb = lamb\n",
    "        self.no_params = sum([self.no_nodes_layer[i]*self.no_nodes_layer[i-1] for i in range(1,len(self.no_nodes_layer))])\n",
    "\n",
    "        \n",
    "        if isinstance(no_nodes_layer, list):\n",
    "            '''\n",
    "            TODO: add functionality so that differnet layers can have different activation functions \n",
    "            '''\n",
    "            assert(self.no_nodes_layer[-1] == 1 or self.no_nodes_layer[-1] == 2)\n",
    "            self.layers = [Layer(self.no_nodes_layer[i-1],self.no_nodes_layer[i],activation_function) for i in range(1,len(self.no_nodes_layer))]\n",
    "            input_layer = Layer(0,no_nodes=self.no_nodes_layer[0],activation_function=activation_function, is_input=True)\n",
    "            self.layers = [input_layer] + self.layers\n",
    "\n",
    "        else:\n",
    "            #Come up with a better default\n",
    "            self.layers = [Layer(no_nodes_layer,1)]\n",
    "\n",
    "        self.W = np.zeros(0)\n",
    "        #turn all the weight matrices into one long weight vector so one can find the l_p norm of it.\n",
    "        self.W = np.concatenate([self.W] +[layer.weights.flatten() for layer in self.layers[1:]])\n",
    "        #print(np.linalg.norm(self.W, 2))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def log_loss(self,X,Y):\n",
    "        y_hats = [self.fwdpropagate(x) for x in X]\n",
    "        J_w_b = (-1)*sum([y*np.log(y_hat)+(1-y)*np.log(1-y_hat) for y,y_hat in zip(Y,y_hats)])\n",
    "        J_w_b += self.lamb*np.linalg.norm(self.W, 2)\n",
    "        return J_w_b\n",
    "        \n",
    "        \n",
    "    def fwdpropagate(self, _input):\n",
    "        \n",
    "        if len(_input) != self.layers[0].no_nodes:\n",
    "            print(f\"Input must be of length {self.layers[0].no_nodes}, it is of length {len(_input)}\")\n",
    "        \n",
    "        self.a_s = []   \n",
    "        a = _input\n",
    "\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            a = layer.fwdprop_output(a)\n",
    "            self.a_s.append(a)\n",
    "        \n",
    "        #self.layer_outputs = []\n",
    "        y_hat = a\n",
    "        #self.layer_outputs.append(y_hat)\n",
    "        \n",
    "        self.prediction = int(np.round(y_hat))\n",
    "\n",
    "        \n",
    "    def d_dout_sig(self, x):\n",
    "        return self.sigmoid(x)*(1.-self.sigmoid(x))\n",
    "    \n",
    "    \n",
    "    def d_dout_loss(self,X):\n",
    "        pass\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        self.fwdpropagate(x_test)\n",
    "        return self.prediction\n",
    "\n",
    "    def stochastic_gradient_descent(self, x_train, y_train, alpha = 0.001, n_iter = 10):\n",
    "        start = time.time()\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        comb = np.c_[x_train.reshape(len(x_train), -1), y_train.reshape(len(y_train), -1)]\n",
    "        \n",
    "        for _ in range(n_iter):\n",
    "            x_train_c = comb[:, :x_train.size//len(x_train)].reshape(x_train.shape)\n",
    "            y_train_c = comb[:, x_train.size//len(x_train):].reshape(y_train.shape)\n",
    "            np.random.shuffle(comb)\n",
    "            score = self.score(x_train,y_train)\n",
    "            print(f\"Training accuracy: {score}%\")\n",
    "            \n",
    "            for x,y in tqdm(zip(x_train_c, y_train_c)):\n",
    "                self.backward_propagate_error(x,y)\n",
    "                for i,layer in enumerate(self.layers):\n",
    "                    if layer.is_input:\n",
    "                        continue\n",
    "                    else:\n",
    "                        layer.weights -= (alpha*np.array(self.delta_w[i]))\n",
    "                        layer.bias -= alpha*np.array(self.delta_b[i])\n",
    "        end = time.time()\n",
    "        print(f\"Ran {n_iter} epochs in {end-start}\")\n",
    "    \n",
    "    def score(self, x_test, y_test):\n",
    "        x_test = np.array(x_test)\n",
    "        y_test = np.array(y_test)\n",
    "        correct = 0\n",
    "        total = len(x_test)\n",
    "        for x,y in zip(x_test, y_test):\n",
    "            self.fwdpropagate(x)\n",
    "            if self.predict(x) ==  y:\n",
    "                correct += 1\n",
    "        return correct/total\n",
    "    \n",
    "    \n",
    "    \n",
    "    def backward_propagate_error(self, x, y):\n",
    "        ''' tick  '''\n",
    "        self.fwdpropagate(x)\n",
    "        delta = [None]*self.no_layers  \n",
    "        delta_w = [None]*self.no_layers \n",
    "        delta_b = [None]*self.no_layers\n",
    "   \n",
    "        for i in range(self.no_layers - 1, -1, -1):\n",
    "            if i == self.no_layers - 1:\n",
    "                if y == 1:\n",
    "                    q = (y/self.layers[i].a)\n",
    "                else:\n",
    "                    q = -1*((1-y)/(1-self.layers[i].a))\n",
    "                delta[i] = -1*q*self.d_dout_sig(self.layers[i].z)\n",
    "            else:\n",
    "                delta[i] = ((self.layers[i+1].weights)@(delta[i+1]))*self.d_dout_sig(self.layers[i].z).reshape(-1,1)\n",
    "                delta_w[i+1] = ((delta[i+1])@(self.layers[i].a.reshape(1,-1))).T + 2*self.lamb*self.layers[i+1].weights\n",
    "                delta_b[i+1] = delta[i+1]\n",
    "\n",
    "        self.delta_w = delta_w\n",
    "        self.delta_b = delta_b\n",
    "        self.delta = delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network on blobs250 and moons400\n",
    "To create the shallow neural network we must create a list with the number of notes in the input layer, number in the hidden layer, the output layer will always be 1. It is neccessary to create a list of the same length with the activation function in the respective layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path+\"blobs250.csv\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class']) \n",
    "\n",
    "acts = [SigmoidActivation, SigmoidActivation, SigmoidActivation]\n",
    "\n",
    "net = Network([3, 10, 1],activation_function=acts, lamb = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "187it [00:00, 7870.58it/s]\n",
      "187it [00:00, 9632.37it/s]\n",
      "187it [00:00, 9088.47it/s]\n",
      "187it [00:00, 9445.72it/s]\n",
      "187it [00:00, 9436.06it/s]\n",
      "187it [00:00, 9201.70it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training accuracy: 0.5348%\n",
      "Running epoch 1 of 10\n",
      "Training accuracy: 0.5348%\n",
      "Running epoch 2 of 10\n",
      "Training accuracy: 0.5936%\n",
      "Running epoch 3 of 10\n",
      "Training accuracy: 0.7005%\n",
      "Running epoch 4 of 10\n",
      "Training accuracy: 0.8075%\n",
      "Running epoch 5 of 10\n",
      "Training accuracy: 0.9519%\n",
      "Running epoch 6 of 10\n",
      "Training accuracy: 1.0000%\n",
      "Running epoch 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "187it [00:00, 7019.54it/s]\n",
      "187it [00:00, 8150.12it/s]\n",
      "187it [00:00, 8884.63it/s]\n",
      "187it [00:00, 8545.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0000%\n",
      "Running epoch 8 of 10\n",
      "Training accuracy: 1.0000%\n",
      "Running epoch 9 of 10\n",
      "Training accuracy: 1.0000%\n",
      "Running epoch 10 of 10\n",
      "Training accuracy: 1.0000%\n",
      "Ran 10 epochs in 0.35856175422668457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([3, 10, 1],activation_function=acts, lamb = 0.001)\n",
    "net.stochastic_gradient_descent(x_train, y_train, alpha=0.001, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 6971.72it/s]\n",
      "300it [00:00, 7498.00it/s]\n",
      "300it [00:00, 9256.57it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training accuracy: 0.6333%\n",
      "Running epoch 1 of 20\n",
      "Training accuracy: 0.6500%\n",
      "Running epoch 2 of 20\n",
      "Training accuracy: 0.7033%\n",
      "Running epoch 3 of 20\n",
      "Training accuracy: 0.7233%\n",
      "Running epoch 4 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 7457.91it/s]\n",
      "300it [00:00, 8523.22it/s]\n",
      "300it [00:00, 8476.94it/s]\n",
      "300it [00:00, 9239.71it/s]\n",
      "300it [00:00, 9215.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7467%\n",
      "Running epoch 5 of 20\n",
      "Training accuracy: 0.7733%\n",
      "Running epoch 6 of 20\n",
      "Training accuracy: 0.7867%\n",
      "Running epoch 7 of 20\n",
      "Training accuracy: 0.7833%\n",
      "Running epoch 8 of 20\n",
      "Training accuracy: 0.7833%\n",
      "Running epoch 9 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 7981.75it/s]\n",
      "300it [00:00, 9068.83it/s]\n",
      "300it [00:00, 8322.91it/s]\n",
      "300it [00:00, 8367.63it/s]\n",
      "300it [00:00, 9192.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8033%\n",
      "Running epoch 10 of 20\n",
      "Training accuracy: 0.8067%\n",
      "Running epoch 11 of 20\n",
      "Training accuracy: 0.8100%\n",
      "Running epoch 12 of 20\n",
      "Training accuracy: 0.8200%\n",
      "Running epoch 13 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9195.01it/s]\n",
      "300it [00:00, 9167.74it/s]\n",
      "300it [00:00, 9205.64it/s]\n",
      "300it [00:00, 7235.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8367%\n",
      "Running epoch 14 of 20\n",
      "Training accuracy: 0.8333%\n",
      "Running epoch 15 of 20\n",
      "Training accuracy: 0.8400%\n",
      "Running epoch 16 of 20\n",
      "Training accuracy: 0.8400%\n",
      "Running epoch 17 of 20\n",
      "Training accuracy: 0.8533%\n",
      "Running epoch 18 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9179.52it/s]\n",
      "300it [00:00, 8236.40it/s]\n",
      "300it [00:00, 8469.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8467%\n",
      "Running epoch 19 of 20\n",
      "Training accuracy: 0.8433%\n",
      "Running epoch 20 of 20\n",
      "Training accuracy: 0.8433%\n",
      "Ran 20 epochs in 1.075195550918579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path+\"moons400.csv\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class']) \n",
    "\n",
    "acts = [SigmoidActivation, SigmoidActivation, SigmoidActivation]\n",
    "\n",
    "net = Network([2, 100, 1],activation_function=acts, lamb = 0.001)\n",
    "\n",
    "net.stochastic_gradient_descent(x_train, y_train, alpha=0.0001, n_iter=20)\n",
    "\n",
    "net.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "There is naturally no improvement in the easier dataset as the logistic regression had 100% test accracy, as did the shallow net. The shallow net improved on the harder of the two data sets by about 6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       | Logistic Regression | Shallow Neural Network |\n",
    "|-------|---------------------|------------------------|\n",
    "| Blobs | 100%                | 100%                   |\n",
    "| Moons | 86%                 | 91%                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9102.09it/s]\n",
      "300it [00:00, 8910.78it/s]\n",
      "300it [00:00, 9568.46it/s]\n",
      "300it [00:00, 7257.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training accuracy: 0.5033%\n",
      "Running epoch 1 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 2 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 3 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 4 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 5 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "300it [00:00, 7700.73it/s]\n",
      "300it [00:00, 8264.10it/s]\n",
      "300it [00:00, 9478.58it/s]\n",
      "300it [00:00, 9404.20it/s]\n",
      "300it [00:00, 9242.76it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 6 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 7 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 8 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 9 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 10 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9368.63it/s]\n",
      "300it [00:00, 7228.98it/s]\n",
      "300it [00:00, 8295.31it/s]\n",
      "300it [00:00, 7924.40it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 11 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 12 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 13 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 14 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9235.50it/s]\n",
      "300it [00:00, 9638.61it/s]\n",
      "300it [00:00, 7987.98it/s]\n",
      "300it [00:00, 9452.95it/s]\n",
      "300it [00:00, 9504.57it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 15 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 16 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 17 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 18 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 19 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9393.25it/s]\n",
      "300it [00:00, 9271.09it/s]\n",
      "300it [00:00, 9432.11it/s]\n",
      "300it [00:00, 8834.83it/s]\n",
      "300it [00:00, 9045.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 20 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 21 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 22 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 23 of 100\n",
      "Training accuracy: 0.5033%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 7926.99it/s]\n",
      "300it [00:00, 8300.40it/s]\n",
      "300it [00:00, 9110.06it/s]\n",
      "300it [00:00, 8938.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running epoch 24 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 25 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 26 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 27 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 28 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9117.59it/s]\n",
      "300it [00:00, 9131.15it/s]\n",
      "300it [00:00, 9526.23it/s]\n",
      "300it [00:00, 9627.47it/s]\n",
      "300it [00:00, 9384.14it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 29 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 30 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 31 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 32 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 33 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9445.14it/s]\n",
      "300it [00:00, 9322.47it/s]\n",
      "300it [00:00, 9579.24it/s]\n",
      "300it [00:00, 9470.95it/s]\n",
      "300it [00:00, 9486.44it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 34 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 35 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 36 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 37 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 38 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9094.66it/s]\n",
      "300it [00:00, 8185.60it/s]\n",
      "300it [00:00, 9170.88it/s]\n",
      "300it [00:00, 9056.95it/s]\n",
      "300it [00:00, 8597.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 39 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 40 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 41 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 42 of 100\n",
      "Training accuracy: 0.5033%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "300it [00:00, 9179.18it/s]\n",
      "300it [00:00, 9358.73it/s]\n",
      "300it [00:00, 8932.22it/s]\n",
      "300it [00:00, 9037.18it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 43 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 44 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 45 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 46 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 47 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9311.09it/s]\n",
      "300it [00:00, 8967.36it/s]\n",
      "300it [00:00, 8383.58it/s]\n",
      "300it [00:00, 9042.31it/s]\n",
      "300it [00:00, 9166.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 48 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 49 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 50 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 51 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 52 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9419.83it/s]\n",
      "300it [00:00, 8745.36it/s]\n",
      "300it [00:00, 9129.96it/s]\n",
      "300it [00:00, 8676.55it/s]\n",
      "300it [00:00, 9480.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 53 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 54 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 55 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 56 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 57 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9591.73it/s]\n",
      "300it [00:00, 9642.15it/s]\n",
      "300it [00:00, 9632.93it/s]\n",
      "300it [00:00, 7794.51it/s]\n",
      "300it [00:00, 9163.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 58 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 59 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 60 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 61 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 62 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9204.63it/s]\n",
      "300it [00:00, 9189.97it/s]\n",
      "300it [00:00, 9452.59it/s]\n",
      "300it [00:00, 9388.13it/s]\n",
      "300it [00:00, 9373.37it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 63 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 64 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 65 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 66 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 67 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 8332.67it/s]\n",
      "300it [00:00, 7812.71it/s]\n",
      "300it [00:00, 9236.59it/s]\n",
      "300it [00:00, 8973.37it/s]\n",
      "300it [00:00, 9253.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 68 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 69 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 70 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 71 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 72 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 7490.72it/s]\n",
      "300it [00:00, 8267.24it/s]\n",
      "300it [00:00, 8985.55it/s]\n",
      "300it [00:00, 7904.19it/s]\n",
      "300it [00:00, 9160.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 73 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 74 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 75 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 76 of 100\n",
      "Training accuracy: 0.5033%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "300it [00:00, 8557.82it/s]\n",
      "300it [00:00, 9182.80it/s]\n",
      "300it [00:00, 8215.75it/s]\n",
      "300it [00:00, 9513.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 77 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 78 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 79 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 80 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 81 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9510.75it/s]\n",
      "300it [00:00, 9529.62it/s]\n",
      "300it [00:00, 9231.78it/s]\n",
      "300it [00:00, 8681.40it/s]\n",
      "300it [00:00, 9288.27it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 82 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 83 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 84 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 85 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 86 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9431.41it/s]\n",
      "300it [00:00, 8849.49it/s]\n",
      "300it [00:00, 9569.55it/s]\n",
      "300it [00:00, 8300.46it/s]\n",
      "300it [00:00, 9600.29it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 87 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 88 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 89 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 90 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 91 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 9491.74it/s]\n",
      "300it [00:00, 8013.06it/s]\n",
      "300it [00:00, 9608.21it/s]\n",
      "300it [00:00, 9648.14it/s]\n",
      "300it [00:00, 8573.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 92 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 93 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 94 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 95 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 8608.94it/s]\n",
      "300it [00:00, 9529.40it/s]\n",
      "300it [00:00, 9463.26it/s]\n",
      "300it [00:00, 8765.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Running epoch 96 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 97 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 98 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 99 of 100\n",
      "Training accuracy: 0.5033%\n",
      "Running epoch 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "300it [00:00, 7991.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5033%\n",
      "Ran 100 epochs in 5.048585891723633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.stochastic_gradient_descent(x_train, y_train, alpha=0.001, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-180-a4ed5ee7fbf7>:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  _all_data = np.array(data)\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='bytes')\n",
    "    return d\n",
    "\n",
    "path_to_images = \"/home/oisin/MAI_work/ongoing_assignments/DeepLearning/cifar-10-batches-py/\"\n",
    "datas = [unpickle(path_to_images+\"data_batch_\"+str(i)) for i in range(1,6)]\n",
    "x = [d[b'data'] for d in datas]\n",
    "y = [d[b'labels'] for d in datas]\n",
    "x_ = np.concatenate(x)\n",
    "y_ = np.concatenate(y)\n",
    "data =  [[imput_x, imput_y - 2] for imput_x, imput_y in zip(x_,y_) if imput_y == 2 or imput_y == 3]\n",
    "\n",
    "_all_data = np.array(data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(_all_data[:,0],_all_data[:,1])\n",
    "inp_size = len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWRITE THE CODE TO CONVERT THESE TO GREYSCALE.\\n\\nFORMULA IS L = R * 299/1000 + G * 587/1000 + B * 114/1000\\nwhere R,G,B are, obviously the RGB colour values.\\n\\nIn the data we have a vector of length 3*1024. Split this into a matrix of size 3x1024 \\nthen convert using the formula\\n\\n'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "WRITE THE CODE TO CONVERT THESE TO GREYSCALE.\n",
    "\n",
    "FORMULA IS L = R * 299/1000 + G * 587/1000 + B * 114/1000\n",
    "where R,G,B are, obviously the RGB colour values.\n",
    "\n",
    "In the data we have a vector of length 3*1024. Split this into a matrix of size 3x1024 \n",
    "then convert using the formula\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'batch_label': b'training batch 1 of 5',\n",
       " b'labels': [6,\n",
       "  9,\n",
       "  9,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  7,\n",
       "  8,\n",
       "  3,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  7,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  9,\n",
       "  5,\n",
       "  7,\n",
       "  9,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  5,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  9,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  7,\n",
       "  9,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  8,\n",
       "  0,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  8,\n",
       "  3,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  8,\n",
       "  5,\n",
       "  2,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  6,\n",
       "  9,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  9,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  8,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  9,\n",
       "  9,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  0,\n",
       "  9,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  7,\n",
       "  0,\n",
       "  4,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  6,\n",
       "  9,\n",
       "  0,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  6,\n",
       "  1,\n",
       "  9,\n",
       "  3,\n",
       "  6,\n",
       "  9,\n",
       "  1,\n",
       "  3,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  2,\n",
       "  9,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  6,\n",
       "  9,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  3,\n",
       "  7,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  8,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  6,\n",
       "  8,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  9,\n",
       "  3,\n",
       "  7,\n",
       "  4,\n",
       "  9,\n",
       "  9,\n",
       "  2,\n",
       "  4,\n",
       "  9,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  9,\n",
       "  0,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  3,\n",
       "  2,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  6,\n",
       "  0,\n",
       "  7,\n",
       "  9,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  9,\n",
       "  9,\n",
       "  0,\n",
       "  8,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  9,\n",
       "  0,\n",
       "  7,\n",
       "  9,\n",
       "  7,\n",
       "  7,\n",
       "  9,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  9,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  8,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  9,\n",
       "  2,\n",
       "  5,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  1,\n",
       "  8,\n",
       "  7,\n",
       "  3,\n",
       "  6,\n",
       "  9,\n",
       "  3,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  8,\n",
       "  5,\n",
       "  4,\n",
       "  7,\n",
       "  2,\n",
       "  3,\n",
       "  9,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  4,\n",
       "  7,\n",
       "  0,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  0,\n",
       "  9,\n",
       "  6,\n",
       "  8,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  8,\n",
       "  9,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  9,\n",
       "  4,\n",
       "  8,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  6,\n",
       "  9,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  4,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  6,\n",
       "  8,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  9,\n",
       "  6,\n",
       "  2,\n",
       "  8,\n",
       "  3,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  9,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  8,\n",
       "  1,\n",
       "  8,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  9,\n",
       "  8,\n",
       "  0,\n",
       "  1,\n",
       "  7,\n",
       "  5,\n",
       "  8,\n",
       "  2,\n",
       "  8,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  9,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  6,\n",
       "  9,\n",
       "  7,\n",
       "  6,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  9,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  8,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  8,\n",
       "  0,\n",
       "  6,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  8,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  8,\n",
       "  7,\n",
       "  1,\n",
       "  9,\n",
       "  1,\n",
       "  9,\n",
       "  8,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  8,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  6,\n",
       "  8,\n",
       "  1,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  9,\n",
       "  9,\n",
       "  0,\n",
       "  9,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  6,\n",
       "  9,\n",
       "  9,\n",
       "  7,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  9,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  0,\n",
       "  9,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  0,\n",
       "  9,\n",
       "  4,\n",
       "  9,\n",
       "  6,\n",
       "  9,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  9,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  9,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  9,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  3,\n",
       "  9,\n",
       "  8,\n",
       "  4,\n",
       "  9,\n",
       "  8,\n",
       "  0,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  6,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  3,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  8,\n",
       "  2,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  0,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  9,\n",
       "  4,\n",
       "  0,\n",
       "  8,\n",
       "  5,\n",
       "  9,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  9,\n",
       "  3,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  2,\n",
       "  7,\n",
       "  9,\n",
       "  0,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  8,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  7,\n",
       "  0,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  0,\n",
       "  9,\n",
       "  4,\n",
       "  3,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  9,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  9,\n",
       "  3,\n",
       "  9,\n",
       "  9,\n",
       "  2,\n",
       "  9,\n",
       "  4,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  8,\n",
       "  0,\n",
       "  6,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  5,\n",
       "  8,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  6,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  6,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  9,\n",
       "  4,\n",
       "  7,\n",
       "  9,\n",
       "  7,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  9,\n",
       "  0,\n",
       "  4,\n",
       "  8,\n",
       "  7,\n",
       "  3,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  2,\n",
       "  8,\n",
       "  5,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  7,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  ...],\n",
       " b'data': array([[ 59,  43,  50, ..., 140,  84,  72],\n",
       "        [154, 126, 105, ..., 139, 142, 144],\n",
       "        [255, 253, 253, ...,  83,  83,  84],\n",
       "        ...,\n",
       "        [ 71,  60,  74, ...,  68,  69,  68],\n",
       "        [250, 254, 211, ..., 215, 255, 254],\n",
       "        [ 62,  61,  60, ..., 130, 130, 131]], dtype=uint8),\n",
       " b'filenames': [b'leptodactylus_pentadactylus_s_000004.png',\n",
       "  b'camion_s_000148.png',\n",
       "  b'tipper_truck_s_001250.png',\n",
       "  b'american_elk_s_001521.png',\n",
       "  b'station_wagon_s_000293.png',\n",
       "  b'coupe_s_001735.png',\n",
       "  b'cassowary_s_001300.png',\n",
       "  b'cow_pony_s_001168.png',\n",
       "  b'sea_boat_s_001584.png',\n",
       "  b'tabby_s_001355.png',\n",
       "  b'muntjac_s_001000.png',\n",
       "  b'arabian_s_001354.png',\n",
       "  b'quarter_horse_s_000672.png',\n",
       "  b'passerine_s_000343.png',\n",
       "  b'camion_s_001895.png',\n",
       "  b'trailer_truck_s_000335.png',\n",
       "  b'dumper_s_000821.png',\n",
       "  b'alley_cat_s_000200.png',\n",
       "  b'accentor_s_000677.png',\n",
       "  b'frog_s_001671.png',\n",
       "  b'capreolus_capreolus_s_000051.png',\n",
       "  b'tomcat_s_000772.png',\n",
       "  b'pickerel_frog_s_000446.png',\n",
       "  b'bufo_s_001242.png',\n",
       "  b'cassowary_s_001246.png',\n",
       "  b'toad_s_001748.png',\n",
       "  b'cat_s_000081.png',\n",
       "  b'chihuahua_s_000825.png',\n",
       "  b'alces_alces_s_000959.png',\n",
       "  b'stealth_bomber_s_000554.png',\n",
       "  b'twinjet_s_000663.png',\n",
       "  b'trucking_rig_s_001402.png',\n",
       "  b'auto_s_000609.png',\n",
       "  b'tabby_cat_s_000983.png',\n",
       "  b'wapiti_s_000416.png',\n",
       "  b'monoplane_s_000895.png',\n",
       "  b'true_cat_s_000247.png',\n",
       "  b'tennessee_walker_s_000486.png',\n",
       "  b'house_cat_s_000243.png',\n",
       "  b'house_cat_s_001196.png',\n",
       "  b'pekinese_s_001337.png',\n",
       "  b'ostrich_s_001368.png',\n",
       "  b'ostrich_s_001150.png',\n",
       "  b'stallion_s_000046.png',\n",
       "  b'station_waggon_s_000041.png',\n",
       "  b'coupe_s_001944.png',\n",
       "  b'estate_car_s_000580.png',\n",
       "  b'accentor_s_000759.png',\n",
       "  b'emu_novaehollandiae_s_000795.png',\n",
       "  b'dive_bomber_s_001390.png',\n",
       "  b'articulated_lorry_s_000131.png',\n",
       "  b'pekinese_s_001093.png',\n",
       "  b'broodmare_s_001463.png',\n",
       "  b'delivery_truck_s_000834.png',\n",
       "  b'songbird_s_001052.png',\n",
       "  b'emu_s_000692.png',\n",
       "  b'puppy_s_000115.png',\n",
       "  b'wagtail_s_001821.png',\n",
       "  b'dama_dama_s_000658.png',\n",
       "  b'domestic_cat_s_001970.png',\n",
       "  b'ambulance_s_003039.png',\n",
       "  b'convertible_s_001763.png',\n",
       "  b'tank_ship_s_001229.png',\n",
       "  b'cassowary_s_001055.png',\n",
       "  b'wagon_s_001142.png',\n",
       "  b'police_cruiser_s_000620.png',\n",
       "  b'moose_s_002308.png',\n",
       "  b'aerial_ladder_truck_s_000584.png',\n",
       "  b'saddle_horse_s_000717.png',\n",
       "  b'tanker_s_001350.png',\n",
       "  b'mongrel_s_001571.png',\n",
       "  b'truck_s_000835.png',\n",
       "  b'pickerel_frog_s_001195.png',\n",
       "  b'lipizzan_s_000399.png',\n",
       "  b'tabby_s_000074.png',\n",
       "  b'automobile_s_001887.png',\n",
       "  b'moving_van_s_001665.png',\n",
       "  b'attack_aircraft_s_000153.png',\n",
       "  b'domestic_cat_s_001596.png',\n",
       "  b'compact_car_s_000048.png',\n",
       "  b'domestic_cat_s_000009.png',\n",
       "  b'pekingese_s_002089.png',\n",
       "  b'capreolus_capreolus_s_001095.png',\n",
       "  b'blenheim_spaniel_s_001103.png',\n",
       "  b'stallion_s_000040.png',\n",
       "  b'stud_mare_s_001672.png',\n",
       "  b'elk_s_000920.png',\n",
       "  b'lipizzan_s_001123.png',\n",
       "  b'fire_truck_s_002721.png',\n",
       "  b'elk_s_001888.png',\n",
       "  b'finch_s_000750.png',\n",
       "  b'tabby_s_000880.png',\n",
       "  b'banana_boat_s_001324.png',\n",
       "  b'twinjet_s_000591.png',\n",
       "  b'shooting_brake_s_000029.png',\n",
       "  b'rana_pipiens_s_000101.png',\n",
       "  b'station_wagon_s_001387.png',\n",
       "  b'station_wagon_s_002712.png',\n",
       "  b'odocoileus_hemionus_s_000221.png',\n",
       "  b'convertible_s_001715.png',\n",
       "  b'abandoned_ship_s_000574.png',\n",
       "  b'true_cat_s_000114.png',\n",
       "  b'dustcart_s_000063.png',\n",
       "  b'frog_s_000797.png',\n",
       "  b'green_frog_s_001320.png',\n",
       "  b'compact_car_s_001038.png',\n",
       "  b'freighter_s_001351.png',\n",
       "  b'mutt_s_000251.png',\n",
       "  b'accentor_s_000303.png',\n",
       "  b'lorry_s_001154.png',\n",
       "  b'fire_truck_s_000894.png',\n",
       "  b'cargo_vessel_s_000731.png',\n",
       "  b'cruiser_s_000774.png',\n",
       "  b'lippizan_s_000359.png',\n",
       "  b'broodmare_s_001741.png',\n",
       "  b'fighter_aircraft_s_000876.png',\n",
       "  b'amphibious_aircraft_s_000216.png',\n",
       "  b'leopard_frog_s_000339.png',\n",
       "  b'trucking_rig_s_001315.png',\n",
       "  b'shooting_brake_s_000886.png',\n",
       "  b'pipit_s_000549.png',\n",
       "  b'ostrich_s_002148.png',\n",
       "  b'trucking_rig_s_001600.png',\n",
       "  b'alauda_arvensis_s_000755.png',\n",
       "  b'rana_temporaria_s_001087.png',\n",
       "  b'bufo_s_000136.png',\n",
       "  b'estate_car_s_000529.png',\n",
       "  b'aerial_ladder_truck_s_000997.png',\n",
       "  b'toy_spaniel_s_000384.png',\n",
       "  b'amphibious_aircraft_s_001195.png',\n",
       "  b'fallow_deer_s_001598.png',\n",
       "  b'quarter_horse_s_000414.png',\n",
       "  b'anuran_s_000712.png',\n",
       "  b'arabian_s_001366.png',\n",
       "  b'auto_s_000040.png',\n",
       "  b'cargo_ship_s_001063.png',\n",
       "  b'motorcar_s_000121.png',\n",
       "  b'motorcar_s_000305.png',\n",
       "  b'anthus_pratensis_s_001071.png',\n",
       "  b'cruiser_s_000294.png',\n",
       "  b'wagon_s_000763.png',\n",
       "  b'alley_cat_s_002579.png',\n",
       "  b'tabby_cat_s_000825.png',\n",
       "  b'spadefoot_s_000051.png',\n",
       "  b'wagtail_s_002075.png',\n",
       "  b'fallow_deer_s_001697.png',\n",
       "  b'garbage_truck_s_000777.png',\n",
       "  b'moving_van_s_000067.png',\n",
       "  b'mongrel_s_000696.png',\n",
       "  b'red_deer_s_000741.png',\n",
       "  b'true_cat_s_001768.png',\n",
       "  b'spadefoot_s_000377.png',\n",
       "  b'lipizzan_s_001233.png',\n",
       "  b'japanese_deer_s_000163.png',\n",
       "  b'toad_s_002436.png',\n",
       "  b'police_boat_s_001421.png',\n",
       "  b'puppy_s_002062.png',\n",
       "  b'lapdog_s_001869.png',\n",
       "  b'elk_s_001196.png',\n",
       "  b'domestic_cat_s_001235.png',\n",
       "  b'car_s_000057.png',\n",
       "  b'lightship_s_000064.png',\n",
       "  b'fallow_deer_s_001997.png',\n",
       "  b'quarter_horse_s_000022.png',\n",
       "  b'bufo_calamita_s_000566.png',\n",
       "  b'biplane_s_000624.png',\n",
       "  b'fire_truck_s_001616.png',\n",
       "  b'japanese_spaniel_s_000037.png',\n",
       "  b'ambulance_s_001394.png',\n",
       "  b'domestic_cat_s_001056.png',\n",
       "  b'tugboat_s_001384.png',\n",
       "  b'emu_s_000072.png',\n",
       "  b'gelding_s_000206.png',\n",
       "  b'puppy_s_000637.png',\n",
       "  b'tabby_cat_s_002607.png',\n",
       "  b'elk_s_001198.png',\n",
       "  b'estate_car_s_001496.png',\n",
       "  b'peke_s_001338.png',\n",
       "  b'arabian_s_001018.png',\n",
       "  b'fighter_aircraft_s_000538.png',\n",
       "  b'fallow_deer_s_001164.png',\n",
       "  b'lippizan_s_000390.png',\n",
       "  b'toy_dog_s_000932.png',\n",
       "  b'maltese_s_002151.png',\n",
       "  b'car_s_000002.png',\n",
       "  b'jumbo_jet_s_000697.png',\n",
       "  b'dump_truck_s_000008.png',\n",
       "  b'spadefoot_s_000084.png',\n",
       "  b'motortruck_s_000014.png',\n",
       "  b'jetliner_s_001502.png',\n",
       "  b'tanker_s_000157.png',\n",
       "  b'riding_horse_s_001870.png',\n",
       "  b'cabin_cruiser_s_000096.png',\n",
       "  b'dredger_s_001623.png',\n",
       "  b'pipit_s_001102.png',\n",
       "  b'mongrel_s_002430.png',\n",
       "  b'skylark_s_000150.png',\n",
       "  b'cat_s_001604.png',\n",
       "  b'puppy_s_000211.png',\n",
       "  b'airbus_s_001124.png',\n",
       "  b'leopard_frog_s_001021.png',\n",
       "  b'wagon_s_001824.png',\n",
       "  b'garbage_truck_s_000147.png',\n",
       "  b'tabby_s_001354.png',\n",
       "  b'rana_temporaria_s_000295.png',\n",
       "  b'lorry_s_000364.png',\n",
       "  b'estate_car_s_000075.png',\n",
       "  b'tabby_s_000970.png',\n",
       "  b'fire_engine_s_000985.png',\n",
       "  b'toad_s_000022.png',\n",
       "  b'texas_toad_s_000215.png',\n",
       "  b'broodmare_s_000707.png',\n",
       "  b'car_s_000286.png',\n",
       "  b'twinjet_s_000565.png',\n",
       "  b'truck_s_001385.png',\n",
       "  b'toy_dog_s_000752.png',\n",
       "  b'police_boat_s_000092.png',\n",
       "  b'toy_dog_s_001197.png',\n",
       "  b'cassowary_s_002446.png',\n",
       "  b'articulated_lorry_s_000050.png',\n",
       "  b'monoplane_s_000781.png',\n",
       "  b'lightship_s_000343.png',\n",
       "  b'powerboat_s_001486.png',\n",
       "  b'jumbo_jet_s_000071.png',\n",
       "  b'bufo_debilis_s_000024.png',\n",
       "  b'transporter_s_000272.png',\n",
       "  b'estate_car_s_000743.png',\n",
       "  b'motorcar_s_000697.png',\n",
       "  b'rana_catesbeiana_s_001655.png',\n",
       "  b'tabby_s_000675.png',\n",
       "  b'remount_s_000285.png',\n",
       "  b'rana_catesbeiana_s_001521.png',\n",
       "  b'true_frog_s_000223.png',\n",
       "  b'airliner_s_002304.png',\n",
       "  b'natterjack_s_000987.png',\n",
       "  b'leptodactylid_s_000082.png',\n",
       "  b'coupe_s_001892.png',\n",
       "  b'quarter_horse_s_000419.png',\n",
       "  b'car_s_001323.png',\n",
       "  b'maltese_dog_s_001043.png',\n",
       "  b'boat_s_000543.png',\n",
       "  b'tabby_cat_s_001537.png',\n",
       "  b'rana_catesbeiana_s_001096.png',\n",
       "  b'bufo_bufo_s_000326.png',\n",
       "  b'pilot_boat_s_000646.png',\n",
       "  b'toad_frog_s_001672.png',\n",
       "  b'boat_s_000988.png',\n",
       "  b'deer_s_001383.png',\n",
       "  b'bufo_bufo_s_000261.png',\n",
       "  b'leopard_frog_s_000763.png',\n",
       "  b'coupe_s_002178.png',\n",
       "  b'cat_s_000663.png',\n",
       "  b'ship_s_000175.png',\n",
       "  b'house_cat_s_001471.png',\n",
       "  b'stag_s_001458.png',\n",
       "  b'station_wagon_s_001337.png',\n",
       "  b'quarter_horse_s_002201.png',\n",
       "  b'coupe_s_000698.png',\n",
       "  b'tomcat_s_002553.png',\n",
       "  b'scow_s_001266.png',\n",
       "  b'chihuahua_s_000385.png',\n",
       "  b'convertible_s_001500.png',\n",
       "  b'convertible_s_000408.png',\n",
       "  b'odocoileus_hemionus_s_000512.png',\n",
       "  b'jumbo_jet_s_000971.png',\n",
       "  b'truck_s_000650.png',\n",
       "  b'domestic_cat_s_001288.png',\n",
       "  b'walking_horse_s_000194.png',\n",
       "  b'elk_s_001657.png',\n",
       "  b'tipper_truck_s_000287.png',\n",
       "  b'dumper_s_000648.png',\n",
       "  b'struthio_camelus_s_000225.png',\n",
       "  b'woodland_caribou_s_000803.png',\n",
       "  b'ladder_truck_s_000795.png',\n",
       "  b'pantechnicon_s_000230.png',\n",
       "  b'shooting_brake_s_000229.png',\n",
       "  b'jetliner_s_001349.png',\n",
       "  b'toy_dog_s_001280.png',\n",
       "  b'dump_truck_s_002104.png',\n",
       "  b'twinjet_s_001360.png',\n",
       "  b'tugboat_s_000715.png',\n",
       "  b'passerine_s_000360.png',\n",
       "  b'automobile_s_002337.png',\n",
       "  b'honey_eater_s_001422.png',\n",
       "  b'attack_aircraft_s_001304.png',\n",
       "  b'toy_dog_s_000114.png',\n",
       "  b'anuran_s_000550.png',\n",
       "  b'mouser_s_000382.png',\n",
       "  b'sparrow_s_000037.png',\n",
       "  b'stud_mare_s_001038.png',\n",
       "  b'speedboat_s_002588.png',\n",
       "  b'passenger_ship_s_002153.png',\n",
       "  b'bufo_bufo_s_001876.png',\n",
       "  b'biplane_s_000651.png',\n",
       "  b'gelding_s_001785.png',\n",
       "  b'trucking_rig_s_001598.png',\n",
       "  b'deer_s_000309.png',\n",
       "  b'pekingese_s_001946.png',\n",
       "  b'frog_s_002802.png',\n",
       "  b'cervus_unicolor_s_000290.png',\n",
       "  b'honey_eater_s_000478.png',\n",
       "  b'automobile_s_001152.png',\n",
       "  b'coupe_s_000846.png',\n",
       "  b'flying_bird_s_000252.png',\n",
       "  b'automobile_s_002343.png',\n",
       "  b'japanese_spaniel_s_001162.png',\n",
       "  b'fire_truck_s_000043.png',\n",
       "  b'semi_s_001207.png',\n",
       "  b'fighter_aircraft_s_001840.png',\n",
       "  b'ship_s_001976.png',\n",
       "  b'caribou_s_001222.png',\n",
       "  b'police_cruiser_s_001347.png',\n",
       "  b'coupe_s_000042.png',\n",
       "  b'bufo_bufo_s_000995.png',\n",
       "  b'tabby_s_001148.png',\n",
       "  b'true_cat_s_000476.png',\n",
       "  b'garbage_truck_s_001442.png',\n",
       "  b'fighter_aircraft_s_000385.png',\n",
       "  b'quarter_horse_s_001602.png',\n",
       "  b'camion_s_000116.png',\n",
       "  b'tennessee_walker_s_000539.png',\n",
       "  b'lipizzan_s_000452.png',\n",
       "  b'trucking_rig_s_001594.png',\n",
       "  b'shooting_brake_s_001611.png',\n",
       "  b'puppy_s_002096.png',\n",
       "  b'auto_s_000855.png',\n",
       "  b'bufo_marinus_s_001364.png',\n",
       "  b'natterjack_s_000036.png',\n",
       "  b'pontoon_s_000643.png',\n",
       "  b'stallion_s_001256.png',\n",
       "  b'coupe_s_001863.png',\n",
       "  b'house_cat_s_000079.png',\n",
       "  b'jetliner_s_000981.png',\n",
       "  b'felis_catus_s_000853.png',\n",
       "  b'tabby_cat_s_000827.png',\n",
       "  b'struthio_camelus_s_001250.png',\n",
       "  b'fawn_s_001915.png',\n",
       "  b'canis_familiaris_s_001298.png',\n",
       "  b'lipizzan_s_002122.png',\n",
       "  b'toy_spaniel_s_001133.png',\n",
       "  b'dump_truck_s_000029.png',\n",
       "  b'fighter_s_001042.png',\n",
       "  b'domestic_cat_s_000493.png',\n",
       "  b'cervus_elaphus_s_001764.png',\n",
       "  b'airbus_s_000627.png',\n",
       "  b'fawn_s_001452.png',\n",
       "  b'fallow_deer_s_000959.png',\n",
       "  b'spadefoot_s_000777.png',\n",
       "  b'seaplane_s_001639.png',\n",
       "  b'plane_s_001066.png',\n",
       "  b'bufo_viridis_s_000348.png',\n",
       "  b'crapaud_s_000051.png',\n",
       "  b'seaplane_s_001447.png',\n",
       "  b'boat_s_000404.png',\n",
       "  b'convertible_s_001242.png',\n",
       "  b'toad_s_000201.png',\n",
       "  b'finch_s_000494.png',\n",
       "  b'fire_engine_s_001437.png',\n",
       "  b'pipit_s_000226.png',\n",
       "  b'toy_dog_s_000960.png',\n",
       "  b'delivery_truck_s_001121.png',\n",
       "  b'bufo_marinus_s_001506.png',\n",
       "  b'studhorse_s_000010.png',\n",
       "  b'woodland_caribou_s_001003.png',\n",
       "  b'station_wagon_s_000260.png',\n",
       "  b'pilot_boat_s_000356.png',\n",
       "  b'stud_mare_s_001497.png',\n",
       "  b'tabby_cat_s_001966.png',\n",
       "  b'american_toad_s_000006.png',\n",
       "  b'tipper_s_000010.png',\n",
       "  b'tomcat_s_002230.png',\n",
       "  b'dive_bomber_s_001120.png',\n",
       "  b'wapiti_s_000802.png',\n",
       "  b'biplane_s_000862.png',\n",
       "  b'blenheim_spaniel_s_000589.png',\n",
       "  b'shooting_brake_s_001161.png',\n",
       "  b'jetliner_s_000400.png',\n",
       "  b'tabby_s_000923.png',\n",
       "  b'alces_alces_s_000361.png',\n",
       "  b'police_boat_s_000002.png',\n",
       "  b'mutt_s_001696.png',\n",
       "  b'cervus_elaphus_s_001695.png',\n",
       "  b'quarter_horse_s_000752.png',\n",
       "  b'accentor_s_000449.png',\n",
       "  b'tabby_cat_s_000144.png',\n",
       "  b'fire_engine_s_001513.png',\n",
       "  b'tennessee_walking_horse_s_001187.png',\n",
       "  b'bufo_s_001247.png',\n",
       "  b'arabian_s_002147.png',\n",
       "  b'compact_s_001258.png',\n",
       "  b'elk_s_002339.png',\n",
       "  b'stud_mare_s_001384.png',\n",
       "  b'fighter_aircraft_s_000745.png',\n",
       "  b'police_cruiser_s_001047.png',\n",
       "  b'lipizzan_s_001764.png',\n",
       "  b'cat_s_000331.png',\n",
       "  b'taxi_s_000701.png',\n",
       "  b'boat_s_000432.png',\n",
       "  b'japanese_deer_s_000942.png',\n",
       "  b'sambar_s_000076.png',\n",
       "  b'ostrich_s_001559.png',\n",
       "  b'dive_bomber_s_000185.png',\n",
       "  b'lark_s_001532.png',\n",
       "  b'flightless_bird_s_000480.png',\n",
       "  b'stealth_fighter_s_001312.png',\n",
       "  b'jumbojet_s_000789.png',\n",
       "  b'delivery_truck_s_000510.png',\n",
       "  b'biplane_s_001901.png',\n",
       "  b'lorry_s_001185.png',\n",
       "  b'crapaud_s_000511.png',\n",
       "  b'boat_s_001280.png',\n",
       "  b'bird_s_001357.png',\n",
       "  b'stud_mare_s_001477.png',\n",
       "  b'arabian_s_000751.png',\n",
       "  b'rangifer_caribou_s_000586.png',\n",
       "  b'monoplane_s_000036.png',\n",
       "  b'house_cat_s_001321.png',\n",
       "  b'stealth_fighter_s_000240.png',\n",
       "  b'oil_tanker_s_000129.png',\n",
       "  b'tipper_truck_s_000416.png',\n",
       "  b'roe_deer_s_000358.png',\n",
       "  b'rhea_americana_s_000475.png',\n",
       "  b'lippizaner_s_000517.png',\n",
       "  b'cassowary_s_000297.png',\n",
       "  b'pekingese_s_001675.png',\n",
       "  b'accentor_s_001058.png',\n",
       "  b'lapdog_s_000227.png',\n",
       "  b'auto_s_001153.png',\n",
       "  b'fire_engine_s_000443.png',\n",
       "  b'muntjac_s_000815.png',\n",
       "  b'lightship_s_000090.png',\n",
       "  b'dog_s_002441.png',\n",
       "  b'auto_s_001951.png',\n",
       "  b'arabian_s_000328.png',\n",
       "  b'odocoileus_hemionus_s_000188.png',\n",
       "  b'barking_deer_s_000019.png',\n",
       "  b'dive_bomber_s_001486.png',\n",
       "  b'bufo_viridis_s_000503.png',\n",
       "  b'wrecker_s_002294.png',\n",
       "  b'monoplane_s_000206.png',\n",
       "  b'arabian_s_002282.png',\n",
       "  b'speedboat_s_002160.png',\n",
       "  b'cargo_vessel_s_002136.png',\n",
       "  b'garbage_truck_s_000726.png',\n",
       "  b'delivery_truck_s_001468.png',\n",
       "  b'house_cat_s_002240.png',\n",
       "  b'house_cat_s_000045.png',\n",
       "  b'caribou_s_002007.png',\n",
       "  b'propeller_plane_s_000451.png',\n",
       "  b'fallow_deer_s_001082.png',\n",
       "  b'dog_s_001897.png',\n",
       "  b'bufo_bufo_s_001631.png',\n",
       "  b'spadefoot_s_000191.png',\n",
       "  b'multiengine_airplane_s_000186.png',\n",
       "  b'auto_s_002359.png',\n",
       "  b'stealth_bomber_s_001607.png',\n",
       "  b'passenger_ship_s_000582.png',\n",
       "  b'airliner_s_001297.png',\n",
       "  b'fallow_deer_s_000973.png',\n",
       "  b'speedboat_s_000178.png',\n",
       "  b'boat_s_001444.png',\n",
       "  b'estate_car_s_000712.png',\n",
       "  b'english_toy_spaniel_s_000888.png',\n",
       "  b'passerine_s_000699.png',\n",
       "  b'rana_pipiens_s_000286.png',\n",
       "  b'tanker_s_000025.png',\n",
       "  b'car_s_001956.png',\n",
       "  b'stealth_bomber_s_000424.png',\n",
       "  b'airliner_s_001070.png',\n",
       "  b'stud_mare_s_001348.png',\n",
       "  b'quarter_horse_s_000282.png',\n",
       "  b'maltese_s_001818.png',\n",
       "  b'lorry_s_001786.png',\n",
       "  b'true_frog_s_000064.png',\n",
       "  b'cassowary_s_001372.png',\n",
       "  b'houseboat_s_000626.png',\n",
       "  b'domestic_cat_s_000035.png',\n",
       "  b'deer_s_001596.png',\n",
       "  b'cow_pony_s_000803.png',\n",
       "  b'tabby_cat_s_001466.png',\n",
       "  b'tipper_truck_s_001525.png',\n",
       "  b'stealth_fighter_s_000533.png',\n",
       "  b'station_wagon_s_000750.png',\n",
       "  b'wagtail_s_001549.png',\n",
       "  b'rangifer_caribou_s_000985.png',\n",
       "  b'cargo_ship_s_000408.png',\n",
       "  b'cab_s_000788.png',\n",
       "  b'flatboat_s_000044.png',\n",
       "  b'bufo_viridis_s_000457.png',\n",
       "  b'fallow_deer_s_000040.png',\n",
       "  b'barren_ground_caribou_s_000077.png',\n",
       "  b'pekinese_s_000453.png',\n",
       "  b'female_horse_s_000358.png',\n",
       "  b'compact_car_s_000869.png',\n",
       "  b'tabby_cat_s_002454.png',\n",
       "  b'delivery_truck_s_000864.png',\n",
       "  b'cargo_ship_s_000802.png',\n",
       "  b'attack_aircraft_s_000037.png',\n",
       "  b'convertible_s_000037.png',\n",
       "  b'broodmare_s_001448.png',\n",
       "  b'toy_spaniel_s_001286.png',\n",
       "  b'packet_boat_s_001206.png',\n",
       "  b'ostrich_s_001564.png',\n",
       "  b'hospital_ship_s_001658.png',\n",
       "  b'stealth_bomber_s_001035.png',\n",
       "  b'stag_s_002583.png',\n",
       "  b'police_cruiser_s_000629.png',\n",
       "  b'merchant_ship_s_001242.png',\n",
       "  b'dump_truck_s_001584.png',\n",
       "  b'cargo_ship_s_001830.png',\n",
       "  b'nandu_s_000721.png',\n",
       "  b'garbage_truck_s_001119.png',\n",
       "  b'lorry_s_002317.png',\n",
       "  b'wagtail_s_001445.png',\n",
       "  b'lipizzan_s_002049.png',\n",
       "  b'maltese_s_000948.png',\n",
       "  b'quarter_horse_s_000798.png',\n",
       "  b'house_cat_s_001472.png',\n",
       "  b'tank_ship_s_001155.png',\n",
       "  b'hospital_ship_s_001838.png',\n",
       "  b'mule_deer_s_000581.png',\n",
       "  b'deer_s_001329.png',\n",
       "  b'rhea_americana_s_000268.png',\n",
       "  b'lippizan_s_000440.png',\n",
       "  b'convertible_s_002117.png',\n",
       "  b'bufo_marinus_s_001567.png',\n",
       "  b'roe_deer_s_000690.png',\n",
       "  b'twinjet_s_000481.png',\n",
       "  b'american_elk_s_001743.png',\n",
       "  b'bufo_viridis_s_000254.png',\n",
       "  b'dumper_s_000179.png',\n",
       "  b'lippizan_s_000323.png',\n",
       "  b'leopard_frog_s_000058.png',\n",
       "  b'prunella_modularis_s_000578.png',\n",
       "  b'maltese_s_001867.png',\n",
       "  b'toy_dog_s_001358.png',\n",
       "  b'police_cruiser_s_000500.png',\n",
       "  b'broodmare_s_001589.png',\n",
       "  b'passerine_s_000381.png',\n",
       "  b'accentor_s_000424.png',\n",
       "  b'finch_s_000617.png',\n",
       "  b'trucking_rig_s_001111.png',\n",
       "  b'canis_familiaris_s_001059.png',\n",
       "  b'cervus_elaphus_s_000813.png',\n",
       "  b'cassowary_s_000339.png',\n",
       "  b'buckskin_s_000325.png',\n",
       "  b'lightship_s_001019.png',\n",
       "  b'shooting_brake_s_001330.png',\n",
       "  b'tomcat_s_002649.png',\n",
       "  b'barren_ground_caribou_s_000174.png',\n",
       "  b'cat_s_000572.png',\n",
       "  b'walking_horse_s_002079.png',\n",
       "  b'rana_temporaria_s_001139.png',\n",
       "  b'tipper_s_000007.png',\n",
       "  b'freighter_s_000661.png',\n",
       "  b'stealth_fighter_s_000020.png',\n",
       "  b'goliath_frog_s_000205.png',\n",
       "  b'stealth_bomber_s_000703.png',\n",
       "  b'prunella_modularis_s_000795.png',\n",
       "  b'struthio_camelus_s_000331.png',\n",
       "  b'dunnock_s_000956.png',\n",
       "  b'auto_s_001284.png',\n",
       "  b'scow_s_000831.png',\n",
       "  b'fallow_deer_s_001256.png',\n",
       "  b'stealth_fighter_s_001297.png',\n",
       "  b'estate_car_s_000845.png',\n",
       "  b'ship_s_001691.png',\n",
       "  b'abandoned_ship_s_000977.png',\n",
       "  b'wagon_s_001298.png',\n",
       "  b'feist_s_000354.png',\n",
       "  b'lipizzan_s_001600.png',\n",
       "  b'leopard_frog_s_001700.png',\n",
       "  b'cervus_elaphus_s_000066.png',\n",
       "  b'toy_s_000517.png',\n",
       "  b'cargo_ship_s_000556.png',\n",
       "  b'stallion_s_001717.png',\n",
       "  b'coupe_s_001004.png',\n",
       "  b'camion_s_001068.png',\n",
       "  b'coupe_s_000001.png',\n",
       "  b'garbage_truck_s_001253.png',\n",
       "  b'sea_boat_s_001960.png',\n",
       "  b'capreolus_capreolus_s_001169.png',\n",
       "  b'lippizan_s_000681.png',\n",
       "  b'tabby_s_000512.png',\n",
       "  b'pontoon_s_000271.png',\n",
       "  b'sea_boat_s_001502.png',\n",
       "  b'lark_s_000700.png',\n",
       "  b'leopard_frog_s_001074.png',\n",
       "  b'rana_palustris_s_000022.png',\n",
       "  b'lippizaner_s_001181.png',\n",
       "  b'wagon_s_000155.png',\n",
       "  b'american_toad_s_001410.png',\n",
       "  b'cargo_vessel_s_000659.png',\n",
       "  b'station_wagon_s_001705.png',\n",
       "  b'transporter_s_000076.png',\n",
       "  b'broodmare_s_001098.png',\n",
       "  b'passenger_ship_s_000050.png',\n",
       "  b'tabby_cat_s_000484.png',\n",
       "  b'fighter_s_000276.png',\n",
       "  b'car_s_000506.png',\n",
       "  b'stealth_fighter_s_000139.png',\n",
       "  b'pleasure_craft_s_000076.png',\n",
       "  b'cruiser_s_000744.png',\n",
       "  b'tabby_cat_s_002478.png',\n",
       "  b'attack_aircraft_s_000423.png',\n",
       "  b'stealth_fighter_s_000460.png',\n",
       "  b'police_cruiser_s_000787.png',\n",
       "  b'mongrel_s_000381.png',\n",
       "  b'attack_aircraft_s_001166.png',\n",
       "  b'supertanker_s_000557.png',\n",
       "  b'tugboat_s_000807.png',\n",
       "  b'stud_mare_s_000413.png',\n",
       "  b'lorry_s_001039.png',\n",
       "  b'trailer_truck_s_001496.png',\n",
       "  b'airliner_s_001477.png',\n",
       "  b'camion_s_001521.png',\n",
       "  b'alces_alces_s_000404.png',\n",
       "  b'automobile_s_000315.png',\n",
       "  b'tabby_cat_s_001542.png',\n",
       "  b'pickerel_frog_s_000003.png',\n",
       "  b'spring_frog_s_000318.png',\n",
       "  b'roebuck_s_000153.png',\n",
       "  b'reindeer_s_000121.png',\n",
       "  b'male_horse_s_000008.png',\n",
       "  b'dog_s_000052.png',\n",
       "  b'bufo_bufo_s_001959.png',\n",
       "  b'stealth_bomber_s_000492.png',\n",
       "  b'pilot_boat_s_001819.png',\n",
       "  b'attack_aircraft_s_001187.png',\n",
       "  b'house_cat_s_000812.png',\n",
       "  b'cassowary_s_000138.png',\n",
       "  b'boat_s_000490.png',\n",
       "  b'wapiti_s_000884.png',\n",
       "  b'bufo_bufo_s_001492.png',\n",
       "  b'delivery_truck_s_000157.png',\n",
       "  b'truck_s_000060.png',\n",
       "  b'stud_mare_s_000506.png',\n",
       "  b'fighter_aircraft_s_000655.png',\n",
       "  b'true_cat_s_000127.png',\n",
       "  b'tabby_s_000228.png',\n",
       "  b'american_green_toad_s_000007.png',\n",
       "  b'quarter_horse_s_001833.png',\n",
       "  b'odocoileus_hemionus_s_001090.png',\n",
       "  b'tipper_lorry_s_000527.png',\n",
       "  b'compact_car_s_001200.png',\n",
       "  b'pickerel_frog_s_000242.png',\n",
       "  b'finch_s_000308.png',\n",
       "  b'arabian_s_002456.png',\n",
       "  b'dunnock_s_001476.png',\n",
       "  b'emu_s_002196.png',\n",
       "  b'plane_s_000065.png',\n",
       "  b'european_toad_s_000360.png',\n",
       "  b'arabian_s_002383.png',\n",
       "  b'dog_s_002098.png',\n",
       "  b'quarter_horse_s_001757.png',\n",
       "  b'rana_temporaria_s_001922.png',\n",
       "  b'cargo_ship_s_001318.png',\n",
       "  b'tip_truck_s_001017.png',\n",
       "  b'biplane_s_001055.png',\n",
       "  b'camion_s_001255.png',\n",
       "  b'red_deer_s_000756.png',\n",
       "  b'capreolus_capreolus_s_000822.png',\n",
       "  b'lippizaner_s_000840.png',\n",
       "  b'propeller_plane_s_001438.png',\n",
       "  b'dumper_s_000646.png',\n",
       "  b'capreolus_capreolus_s_000284.png',\n",
       "  b'truck_s_000526.png',\n",
       "  b'bufo_s_001473.png',\n",
       "  b'fire_engine_s_001449.png',\n",
       "  b'elk_s_001164.png',\n",
       "  b'canis_familiaris_s_001319.png',\n",
       "  b'tennessee_walking_horse_s_000005.png',\n",
       "  b'truck_s_000838.png',\n",
       "  b'rhea_s_001649.png',\n",
       "  b'mule_deer_s_002514.png',\n",
       "  b'canis_familiaris_s_000143.png',\n",
       "  b'police_cruiser_s_001073.png',\n",
       "  b'cervus_sika_s_000273.png',\n",
       "  b'cat_s_001584.png',\n",
       "  b'tipper_lorry_s_000313.png',\n",
       "  b'bufo_americanus_s_001202.png',\n",
       "  b'english_toy_spaniel_s_000735.png',\n",
       "  b'bufo_calamita_s_000043.png',\n",
       "  b'tipper_s_000180.png',\n",
       "  b'tabby_cat_s_000881.png',\n",
       "  b'tabby_cat_s_001644.png',\n",
       "  b'blenheim_spaniel_s_000330.png',\n",
       "  b'attack_aircraft_s_001138.png',\n",
       "  b'arabian_s_002270.png',\n",
       "  b'cassowary_s_000845.png',\n",
       "  b'wagon_s_000692.png',\n",
       "  b'tabby_cat_s_000319.png',\n",
       "  b'american_toad_s_000077.png',\n",
       "  b'moose_s_001500.png',\n",
       "  b'airliner_s_001406.png',\n",
       "  b'fighter_aircraft_s_000966.png',\n",
       "  b'dunnock_s_000055.png',\n",
       "  b'pekingese_s_002399.png',\n",
       "  b'twinjet_s_000739.png',\n",
       "  b'automobile_s_002249.png',\n",
       "  b'airliner_s_000669.png',\n",
       "  b'wagtail_s_001656.png',\n",
       "  b'tabby_s_001949.png',\n",
       "  b'dump_truck_s_001009.png',\n",
       "  b'freighter_s_000436.png',\n",
       "  b'fallow_deer_s_001171.png',\n",
       "  b'trucking_rig_s_000684.png',\n",
       "  b'ship_s_000249.png',\n",
       "  b'monoplane_s_000416.png',\n",
       "  b'songbird_s_002066.png',\n",
       "  b'rana_catesbeiana_s_000602.png',\n",
       "  b'red_deer_s_000536.png',\n",
       "  b'elk_s_001583.png',\n",
       "  b'stealth_fighter_s_001313.png',\n",
       "  b'station_wagon_s_002498.png',\n",
       "  b'pleasure_boat_s_001949.png',\n",
       "  b'cargo_ship_s_000060.png',\n",
       "  b'tabby_cat_s_001767.png',\n",
       "  b'crapaud_s_000055.png',\n",
       "  b'dump_truck_s_002008.png',\n",
       "  b'bufo_viridis_s_000506.png',\n",
       "  b'bullfrog_s_001295.png',\n",
       "  b'riding_horse_s_001884.png',\n",
       "  b'boat_s_002344.png',\n",
       "  b'ostrich_s_000950.png',\n",
       "  b'muntjac_s_000681.png',\n",
       "  b'pekingese_s_000279.png',\n",
       "  b'lipizzan_s_002052.png',\n",
       "  b'rana_pipiens_s_000570.png',\n",
       "  b'toy_s_002250.png',\n",
       "  b'tabby_s_001940.png',\n",
       "  b'dive_bomber_s_001294.png',\n",
       "  b'mutt_s_000086.png',\n",
       "  b'biplane_s_000701.png',\n",
       "  b'toy_s_000964.png',\n",
       "  b'jetliner_s_000807.png',\n",
       "  b'barge_s_000426.png',\n",
       "  b'cassowary_s_000255.png',\n",
       "  b'rana_clamitans_s_000059.png',\n",
       "  b'quarter_horse_s_000222.png',\n",
       "  b'tomcat_s_000377.png',\n",
       "  b'scow_s_000687.png',\n",
       "  b'emu_s_001571.png',\n",
       "  b'coupe_s_001625.png',\n",
       "  b'roan_s_000199.png',\n",
       "  b'bufo_bufo_s_000626.png',\n",
       "  b'tennessee_walking_horse_s_001470.png',\n",
       "  b'station_wagon_s_001497.png',\n",
       "  b'jumbo_jet_s_001056.png',\n",
       "  b'trailer_truck_s_000099.png',\n",
       "  b'puppy_s_000502.png',\n",
       "  b'pekingese_s_002101.png',\n",
       "  b'propeller_plane_s_001193.png',\n",
       "  b'car_s_001009.png',\n",
       "  b'lipizzan_s_002011.png',\n",
       "  b'green_frog_s_001375.png',\n",
       "  b'tipper_lorry_s_000586.png',\n",
       "  b'jetliner_s_000509.png',\n",
       "  b'musk_deer_s_000211.png',\n",
       "  b'appaloosa_s_002251.png',\n",
       "  b'arabian_s_001642.png',\n",
       "  b'compact_car_s_000435.png',\n",
       "  b'chihuahua_s_000063.png',\n",
       "  b'semi_s_000462.png',\n",
       "  b'wapiti_s_000455.png',\n",
       "  b'twinjet_s_001120.png',\n",
       "  b'cargo_vessel_s_001235.png',\n",
       "  b'peke_s_000274.png',\n",
       "  b'dump_truck_s_001422.png',\n",
       "  b'dump_truck_s_001863.png',\n",
       "  b'tailed_frog_s_000638.png',\n",
       "  b'gelding_s_000965.png',\n",
       "  b'coupe_s_000056.png',\n",
       "  b'pleasure_craft_s_000625.png',\n",
       "  b'house_cat_s_001328.png',\n",
       "  b'struthio_camelus_s_000081.png',\n",
       "  b'domestic_cat_s_000051.png',\n",
       "  b'passenger_ship_s_000948.png',\n",
       "  b'sparrow_s_002697.png',\n",
       "  b'prunella_modularis_s_000302.png',\n",
       "  b'fallow_deer_s_000192.png',\n",
       "  b'rana_palustris_s_000107.png',\n",
       "  b'attack_aircraft_s_001075.png',\n",
       "  b'twinjet_s_000046.png',\n",
       "  b'chihuahua_s_001038.png',\n",
       "  b'tabby_cat_s_001004.png',\n",
       "  b'cargo_vessel_s_001812.png',\n",
       "  b'dunnock_s_000037.png',\n",
       "  b'tabby_s_001740.png',\n",
       "  b'gelding_s_001638.png',\n",
       "  b'anthus_pratensis_s_001166.png',\n",
       "  b'camion_s_001988.png',\n",
       "  b'alley_cat_s_001645.png',\n",
       "  b'oil_tanker_s_001626.png',\n",
       "  b'tennessee_walking_horse_s_000355.png',\n",
       "  b'containership_s_000812.png',\n",
       "  b'passerine_s_000801.png',\n",
       "  b'lipizzan_s_000635.png',\n",
       "  b'truck_s_000635.png',\n",
       "  b'twinjet_s_000450.png',\n",
       "  b'honey_eater_s_000702.png',\n",
       "  b'tabby_s_000039.png',\n",
       "  b'pipit_s_000827.png',\n",
       "  b'elephant_bird_s_001004.png',\n",
       "  b'night_bird_s_000241.png',\n",
       "  b'alley_cat_s_001301.png',\n",
       "  b'house_cat_s_000332.png',\n",
       "  b'bufo_americanus_s_000492.png',\n",
       "  b'wagtail_s_001443.png',\n",
       "  b'tabby_cat_s_000587.png',\n",
       "  b'gamecock_s_000098.png',\n",
       "  b'cargo_ship_s_001867.png',\n",
       "  b'seaplane_s_000416.png',\n",
       "  b'english_toy_spaniel_s_001058.png',\n",
       "  b'japanese_spaniel_s_001324.png',\n",
       "  b'coupe_s_000179.png',\n",
       "  b'american_elk_s_000201.png',\n",
       "  b'chihuahua_s_000666.png',\n",
       "  b'spadefoot_s_000219.png',\n",
       "  b'bufo_s_001733.png',\n",
       "  b'struthio_camelus_s_001368.png',\n",
       "  b'arab_s_000456.png',\n",
       "  b'monoplane_s_001154.png',\n",
       "  b'coupe_s_001690.png',\n",
       "  b'dawn_horse_s_000752.png',\n",
       "  b'tennessee_walking_horse_s_000013.png',\n",
       "  b'cabin_cruiser_s_001500.png',\n",
       "  b'sparrow_s_001750.png',\n",
       "  b'aerial_ladder_truck_s_001306.png',\n",
       "  b'dromaius_novaehollandiae_s_000433.png',\n",
       "  b'passerine_s_000376.png',\n",
       "  b'caribou_s_001447.png',\n",
       "  b'bird_s_000356.png',\n",
       "  b'motorcar_s_001389.png',\n",
       "  b'auto_s_000675.png',\n",
       "  b'automobile_s_001833.png',\n",
       "  b'rana_temporaria_s_000008.png',\n",
       "  b'spring_frog_s_001263.png',\n",
       "  b'barking_frog_s_000397.png',\n",
       "  b'puppy_s_000785.png',\n",
       "  b'convertible_s_001861.png',\n",
       "  b'convertible_s_000859.png',\n",
       "  b'american_saddle_horse_s_000039.png',\n",
       "  b'biplane_s_000244.png',\n",
       "  b'red_deer_s_000704.png',\n",
       "  b'tabby_s_001071.png',\n",
       "  b'tabby_cat_s_002548.png',\n",
       "  b'tennessee_walker_s_001791.png',\n",
       "  b'auto_s_002048.png',\n",
       "  b'dromaius_novaehollandiae_s_000296.png',\n",
       "  b'felis_catus_s_001460.png',\n",
       "  b'peke_s_000440.png',\n",
       "  b'chihuahua_s_000349.png',\n",
       "  b'lapdog_s_002195.png',\n",
       "  b'bufo_marinus_s_001243.png',\n",
       "  b'convertible_s_001433.png',\n",
       "  b'mule_deer_s_000628.png',\n",
       "  b'tabby_s_000410.png',\n",
       "  b'stud_s_000099.png',\n",
       "  b'passenger_ship_s_001530.png',\n",
       "  b'scow_s_000131.png',\n",
       "  b'tabby_s_001868.png',\n",
       "  b'bufo_bufo_s_002334.png',\n",
       "  b'bufo_viridis_s_000152.png',\n",
       "  b'passerine_s_001306.png',\n",
       "  b'house_cat_s_002116.png',\n",
       "  b'jetliner_s_001188.png',\n",
       "  b'moving_van_s_001359.png',\n",
       "  b'wapiti_s_000663.png',\n",
       "  b'felis_catus_s_000998.png',\n",
       "  b'sea_boat_s_001979.png',\n",
       "  b'fighter_aircraft_s_001776.png',\n",
       "  b'fighter_aircraft_s_001096.png',\n",
       "  b'shooting_brake_s_000774.png',\n",
       "  b'automobile_s_001310.png',\n",
       "  b'pekingese_s_001023.png',\n",
       "  b'american_elk_s_000455.png',\n",
       "  b'trucking_rig_s_001243.png',\n",
       "  b'house_cat_s_000877.png',\n",
       "  b'taxi_s_000040.png',\n",
       "  b'pilot_boat_s_001100.png',\n",
       "  b'articulated_lorry_s_000223.png',\n",
       "  b'felis_domesticus_s_000164.png',\n",
       "  b'trucking_rig_s_001300.png',\n",
       "  b'tractor_trailer_s_000254.png',\n",
       "  b'flightless_bird_s_000142.png',\n",
       "  b'dump_truck_s_000939.png',\n",
       "  b'deer_s_000967.png',\n",
       "  b'ship_s_001904.png',\n",
       "  b'wagtail_s_000309.png',\n",
       "  b'trailer_truck_s_000815.png',\n",
       "  b'container_ship_s_002477.png',\n",
       "  b'guard_boat_s_000285.png',\n",
       "  b'estate_car_s_000446.png',\n",
       "  b'puppy_s_001621.png',\n",
       "  b'tabby_cat_s_000689.png',\n",
       "  b'american_toad_s_000650.png',\n",
       "  b'boat_s_002381.png',\n",
       "  b'female_horse_s_000089.png',\n",
       "  b'bufo_viridis_s_001370.png',\n",
       "  b'ladder_truck_s_000044.png',\n",
       "  b'cargo_ship_s_001321.png',\n",
       "  b'twinjet_s_000547.png',\n",
       "  b'cascades_frog_s_000061.png',\n",
       "  b'odocoileus_hemionus_s_001017.png',\n",
       "  b'attack_aircraft_s_001254.png',\n",
       "  b'airplane_s_000006.png',\n",
       "  b'songbird_s_001232.png',\n",
       "  b'peke_s_000388.png',\n",
       "  b'boat_s_002143.png',\n",
       "  b'accentor_s_001182.png',\n",
       "  b'dive_bomber_s_001066.png',\n",
       "  b'dunnock_s_001650.png',\n",
       "  b'dawn_horse_s_001127.png',\n",
       "  b'european_toad_s_000359.png',\n",
       "  b'wrecker_s_002325.png',\n",
       "  b'lipizzan_s_000118.png',\n",
       "  b'automobile_s_000327.png',\n",
       "  b'pekinese_s_002027.png',\n",
       "  b'maltese_s_001913.png',\n",
       "  b'toad_frog_s_000836.png',\n",
       "  b'leopard_frog_s_001231.png',\n",
       "  b'domestic_cat_s_001066.png',\n",
       "  b'barking_frog_s_000840.png',\n",
       "  b'finch_s_000038.png',\n",
       "  b'capreolus_capreolus_s_000257.png',\n",
       "  b'lippizan_s_000126.png',\n",
       "  b'attack_aircraft_s_000089.png',\n",
       "  b'mutt_s_002278.png',\n",
       "  b'true_toad_s_000021.png',\n",
       "  b'capreolus_capreolus_s_000178.png',\n",
       "  b'bufo_viridis_s_000600.png',\n",
       "  b'chihuahua_s_000097.png',\n",
       "  b'meadow_pipit_s_001282.png',\n",
       "  b'red_deer_s_000008.png',\n",
       "  b'leopard_frog_s_001528.png',\n",
       "  b'estate_car_s_001220.png',\n",
       "  b'bullfrog_s_000408.png',\n",
       "  b'stealth_fighter_s_001637.png',\n",
       "  b'capreolus_capreolus_s_001669.png',\n",
       "  b'airbus_s_000736.png',\n",
       "  b'tabby_s_000292.png',\n",
       "  b'station_wagon_s_002793.png',\n",
       "  b'boat_s_000765.png',\n",
       "  b'english_toy_spaniel_s_000814.png',\n",
       "  b'european_elk_s_000265.png',\n",
       "  b'roe_deer_s_000371.png',\n",
       "  b'convertible_s_000414.png',\n",
       "  b'cow_pony_s_000145.png',\n",
       "  b'tabby_s_000126.png',\n",
       "  b'trucking_rig_s_001628.png',\n",
       "  b'muntjac_s_001738.png',\n",
       "  b'broodmare_s_000582.png',\n",
       "  b'fire_engine_s_001540.png',\n",
       "  b'gelding_s_000640.png',\n",
       "  b'tabby_cat_s_002533.png',\n",
       "  b'appaloosa_s_000342.png',\n",
       "  b'pipit_s_001066.png',\n",
       "  b'speedboat_s_000410.png',\n",
       "  b'roe_deer_s_001014.png',\n",
       "  b'american_toad_s_001815.png',\n",
       "  b'bufo_calamita_s_000773.png',\n",
       "  b'estate_car_s_000044.png',\n",
       "  b'cassowary_s_000021.png',\n",
       "  b'lorry_s_001709.png',\n",
       "  b'amphibious_aircraft_s_000141.png',\n",
       "  b'capreolus_capreolus_s_000073.png',\n",
       "  b'motorboat_s_000773.png',\n",
       "  b'lipizzan_s_001658.png',\n",
       "  b'tabby_cat_s_000531.png',\n",
       "  b'truck_s_000289.png',\n",
       "  b'speedboat_s_001711.png',\n",
       "  b'quarter_horse_s_000557.png',\n",
       "  b'quarter_horse_s_000942.png',\n",
       "  b'jumbo_jet_s_001476.png',\n",
       "  b'struthio_camelus_s_001197.png',\n",
       "  b'fallow_deer_s_001623.png',\n",
       "  b'compact_car_s_000850.png',\n",
       "  b'coupe_s_001912.png',\n",
       "  b'roe_deer_s_000678.png',\n",
       "  b'car_s_000674.png',\n",
       "  b'chihuahua_s_001937.png',\n",
       "  b'red_deer_s_002698.png',\n",
       "  b'jumbojet_s_001587.png',\n",
       "  b'toy_spaniel_s_001692.png',\n",
       "  b'rana_clamitans_s_000766.png',\n",
       "  b'pipit_s_000025.png',\n",
       "  b'cargo_vessel_s_001809.png',\n",
       "  b'pekingese_s_001613.png',\n",
       "  b'dive_bomber_s_000707.png',\n",
       "  b'lark_s_000550.png',\n",
       "  b'estate_car_s_000292.png',\n",
       "  b'tabby_cat_s_001853.png',\n",
       "  b'lapdog_s_001963.png',\n",
       "  b'palfrey_s_000300.png',\n",
       "  b'cat_s_002298.png',\n",
       "  b'mutt_s_000188.png',\n",
       "  b'police_cruiser_s_000471.png',\n",
       "  b'true_cat_s_001598.png',\n",
       "  b'peke_s_000638.png',\n",
       "  ...]}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acts = [ReluActivation, ReluActivation, ReluActivation, ReluActivation, SigmoidActivation]\n",
    "acts = lambda x:1/(1+np.exp(-x))\n",
    "nn = Network([inp_size,inp_size//2, inp_size//4, 1],activation_function=acts, lamb = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5899008"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.no_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-cad4315dd69e>:2: RuntimeWarning: overflow encountered in exp\n",
      "  acts = lambda x:1/(1+np.exp(-x))\n",
      "0it [00:00, ?it/s]<ipython-input-5-fd9d4c937ce1>:58: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n",
      "1it [00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.602%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7500it [13:47,  9.07it/s]\n",
      "1it [00:00,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6492%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:03,  9.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-59b43f560958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-fd9d4c937ce1>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[0;34m(self, x_train, y_train, alpha, n_iter)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagate_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-fd9d4c937ce1>\u001b[0m in \u001b[0;36mbackward_propagate_error\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_dout_sig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mdelta_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0mdelta_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn.stochastic_gradient_descent(np.array(x_train), np.array(y_train), alpha=0.001, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-cad4315dd69e>:2: RuntimeWarning: overflow encountered in exp\n",
      "  acts = lambda x:1/(1+np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6204"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.delta[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='bytes')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='bytes')\n",
    "    return d\n",
    "\n",
    "path_to_images = \"/home/oisin/MAI_work/ongoing_assignments/DeepLearning/cifar-10-batches-py/\"\n",
    "datas = [unpickle(path_to_images+\"data_batch_\"+str(i)) for i in range(1,6)]\n",
    "x = [d[b'data'] for d in datas]\n",
    "y = [d[b'labels'] for d in datas]\n",
    "x_ = np.concatenate(x)\n",
    "y_ = np.concatenate(y)\n",
    "data =  [[imput_x, imput_y - 2] for imput_x, imput_y in zip(x_,y_) if imput_y == 2 or imput_y == 3]\n",
    "\n",
    "_all_data = np.array(data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(_all_data[:,0],_all_data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-164-fd60496cca29>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  _all_data = np.array(data)\n"
     ]
    }
   ],
   "source": [
    "_all_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(_all_data[:,0],_all_data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_shape = len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acts = [ReluActivation, ReluActivation, ReluActivation, SigmoidActivation]\n",
    "acts = lambda x : 1/(1+np.exp(-x))\n",
    "net = Network([imp_shape,100,100,1],activation_function=acts, lamb = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 0 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-144-bfc37fa482b7>:54: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n",
      "17it [00:00, 166.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.49666666666666665%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7500it [00:38, 195.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:00, 163.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.49666666666666665%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7500it [00:42, 177.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 2 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:00, 168.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.49666666666666665%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7500it [00:39, 190.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 3 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:00, 164.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.49666666666666665%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4130it [00:21, 190.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-30acb5095279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-bfc37fa482b7>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[0;34m(self, x_train, y_train, alpha, n_iter)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagate_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-bfc37fa482b7>\u001b[0m in \u001b[0;36mbackward_propagate_error\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mdelta_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0mdelta_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.stochastic_gradient_descent(x_train,y_train, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-144-bfc37fa482b7>:54: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5096"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.delta[-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
