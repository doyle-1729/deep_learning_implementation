{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To left justify the tables in markdown so they are nicer to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression implementation.\n",
    "The implementation of logistic regression here is as described in the lecture notes. The weights are created as a vector, each uniformly distributed on $[0,1]$. The 'back propogation' step takes place in the stochastic_gradient_descent function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_inputs, sgd_thresh = 10e-8, sgd_max_iters = 2500, alpha = 0.0001):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.weights = np.random.rand(self.n_inputs, 1).astype('f')\n",
    "        #print(self.weights)\n",
    "        self.bias = 1.0\n",
    "        self.alpha = alpha\n",
    "        self.sgd_thresh = sgd_thresh\n",
    "        self.sgd_max_iters = sgd_max_iters\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.array(x).reshape(-1,1)\n",
    "        return int(np.round(self.sigmoid(np.dot(self.weights.T, x) + self.bias)))\n",
    "    \n",
    "    def log_loss(self, X, Y):\n",
    "        #dot = [np.dot(self.weights.T, x) for x in X]\n",
    "        \n",
    "        y_hats = [self.sigmoid(np.dot(self.weights.T, x) + self.bias) for x in X]\n",
    "        self.yhats = y_hats\n",
    "        #print(np.sum(y_hats))\n",
    "        J_w_b = (-1/len(Y))*np.sum([y*np.log(y_hat)+(1-y)*np.log(1-y_hat) for y,y_hat in zip(Y,y_hats)])\n",
    "        return J_w_b\n",
    "\n",
    "    def stochastic_gradient_descent(self,X,Y):\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        J = self.log_loss(X,Y)\n",
    "        for i in range(self.sgd_max_iters):\n",
    "            idx = np.random.randint(len(X))\n",
    "            x_sample, y_sample = X[idx], Y[idx]\n",
    "            diff = self.predict(x_sample) - y_sample\n",
    "\n",
    "            self.weights = self.weights - (self.alpha*diff*x_sample).reshape(-1,1)\n",
    "\n",
    "            self.bias = self.bias - self.alpha*diff\n",
    "\n",
    "            \n",
    "            if self.log_loss(X,Y) - J < self.sgd_thresh:\n",
    "                break\n",
    "            #print(self.log_loss(X,Y), J)\n",
    "            J = self.log_loss(X,Y)\n",
    "            \n",
    "    def score(self, x_test, y_test):\n",
    "        x_test = np.array(x_test)\n",
    "        y_test = np.array(y_test)\n",
    "        correct = 0\n",
    "        total = len(x_test)\n",
    "        for x,y in zip(x_test, y_test):\n",
    "            if self.predict(x) ==  y:\n",
    "                correct += 1\n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the blobs250 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe then on the easiest of all the data sets that logistic regression perfectly splits the datasets and scores 100% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path+\"blobs250.csv\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class']) \n",
    "\n",
    "lr = LogisticRegression(x_train.shape[1],sgd_max_iters=200,sgd_thresh=-np.inf,alpha=10e-2)\n",
    "lr.stochastic_gradient_descent(x_train,y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the moons400 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded in using pandas and is split into training and test sets using the train_test_split function from the model_selection module in the sci-kit learn package. On the harder of the two problems it achieves an accuracy of 84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/oisin/MAI_work/ongoing_assignments/DeepLearning/data/\"\n",
    "\n",
    "data = pd.read_csv(path+\"moons400.csv\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class']) \n",
    "\n",
    "lr = LogisticRegression(x_train.shape[1],sgd_max_iters=200,sgd_thresh=-np.inf,alpha=10e-2)\n",
    "lr.stochastic_gradient_descent(x_train,y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network Performance\n",
    "Below is the code for the implementation of the neural network. Since one of my enhancements was to implement deep neural networks (support for multiple layers), the code is identical to the enhancement code. This is simply because there was no point implementing code specific to one hidden layer and then implementing code for many layers when just generalising first does the trick. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Class\n",
    "This implementation has a Layer class, where the weights and layer activation function are stored. The weights are initially $\\mathcal{N}(0,1)$ as are the biases. The class implements \"fwdprop_ouput\" which calculates a and z for the Layer, as seen in the notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Class\n",
    "The Network class creates the layers, as directed by the inputs. These inputs are in the form of 2 lists, m and n say, where the $i^{th}$ member of m is the number of nodes in layer $i$, and the $i^{th}$ member of n is the activation function class for that layer (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function Class\n",
    "The activation functions are created with an \"f\" method and a \"deriv\" method. These represent the function itself and the derivative of the function, to be used in the back propagation step, as required by the chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, prev_no_nodes, no_nodes, activation_function, is_input=False):\n",
    "        if not(is_input):\n",
    "            bound = np.sqrt(6)/np.sqrt(prev_no_nodes + no_nodes)\n",
    "            self.weights = np.random.uniform(-1*bound,bound,size = (prev_no_nodes,no_nodes))\n",
    "            self.bias = np.random.normal(-1*bound,bound, size = (no_nodes,1))\n",
    "            self.act_f = activation_function()\n",
    "            \n",
    "        else:\n",
    "            self.act_f = activation_function()\n",
    "        self.no_nodes = no_nodes\n",
    "        self.is_input = is_input\n",
    "\n",
    "    def fwdprop_output(self, X):\n",
    "        if self.is_input:\n",
    "            self.a = X\n",
    "            self.z = self.a\n",
    "            return X\n",
    "        X = X.reshape(-1,1)\n",
    "        self.z = np.dot(self.weights.T,X) + self.bias\n",
    "        self.a = self.act_f.f(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    \n",
    "    \n",
    "class IdentityActivation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def f(self,x):\n",
    "        return x\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ReluActivation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def f(self, x):\n",
    "        return np.maximum(x,0)\n",
    "        \n",
    "    def deriv(self, x):\n",
    "        x[x>0] = 1\n",
    "        x[x<0] = 0\n",
    "        return x\n",
    "        \n",
    "class SigmoidActivation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def f(self,x):\n",
    "        x=x.astype('f')\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def deriv(self, x):\n",
    "        return self.f(x)*(1-self.f(x))\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, no_nodes_layer, activation_function, loss_function = \"log_loss\", lamb = 0):\n",
    "        '''\n",
    "        TODO: Sort out the fact that the first layer doesn't have an activation function (DOESNT FUCKIN NEED ONE AHAHHA)\n",
    "        TODO: Add the loss_functions\n",
    "        TODO: Finish train and predict/score\n",
    "        '''\n",
    "        self.no_nodes_layer = no_nodes_layer\n",
    "        self.activation_function = activation_function\n",
    "        self.input_size = no_nodes_layer[0]\n",
    "        self.no_layers = len(self.no_nodes_layer)\n",
    "        self.lamb = lamb\n",
    "        self.no_params = sum([self.no_nodes_layer[i]*self.no_nodes_layer[i-1] for i in range(1,len(self.no_nodes_layer))])\n",
    "        \n",
    "        if isinstance(no_nodes_layer, list):\n",
    "            '''\n",
    "            TODO: add functionality so that differnet layers can have different activation functions \n",
    "            '''\n",
    "            assert(self.no_nodes_layer[-1] == 1 or self.no_nodes_layer[-1] == 2)\n",
    "            self.layers = [Layer(self.no_nodes_layer[i-1],self.no_nodes_layer[i],activation_function[i]) for i in range(1,len(self.no_nodes_layer))]\n",
    "            input_layer = Layer(0,no_nodes=self.no_nodes_layer[0],activation_function=activation_function[0], is_input=True)\n",
    "            self.layers = [input_layer] + self.layers\n",
    "\n",
    "        else:\n",
    "            #Come up with a better default\n",
    "            self.layers = [Layer(no_nodes_layer,1)]\n",
    "\n",
    "        self.W = np.zeros(0)\n",
    "        #turn all the weight matrices into one long weight vector so one can find the l_p norm of it.\n",
    "        self.W = np.concatenate([self.W] +[layer.weights.flatten() for layer in self.layers[1:]])\n",
    "        #print(np.linalg.norm(self.W, 2))\n",
    "    \n",
    "\n",
    "    \n",
    "    def log_loss(self,X,Y):\n",
    "        y_hats = [self.fwdpropagate(x) for x in X]\n",
    "        J_w_b = (-1)*sum([y*np.log(y_hat)+(1-y)*np.log(1-y_hat) for y,y_hat in zip(Y,y_hats)])\n",
    "        J_w_b += self.lamb*np.linalg.norm(self.W, 2)\n",
    "        return J_w_b\n",
    "        \n",
    "        \n",
    "    def fwdpropagate(self, _input):\n",
    "        \n",
    "        if len(_input) != self.layers[0].no_nodes:\n",
    "            print(f\"Input must be of length {self.layers[0].no_nodes}, it is of length {len(_input)}\")\n",
    "        \n",
    "        self.a_s = []   \n",
    "        a = _input\n",
    "\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            a = layer.fwdprop_output(a)\n",
    "            self.a_s.append(a)\n",
    "        \n",
    "        #self.layer_outputs = []\n",
    "        y_hat = a\n",
    "        #self.layer_outputs.append(y_hat)\n",
    "        \n",
    "        self.prediction = int(np.round(y_hat))\n",
    "\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        self.fwdpropagate(x_test)\n",
    "        return self.prediction\n",
    "\n",
    "    def stochastic_gradient_descent(self, x_train, y_train, alpha = 0.001, n_iter = 10, val_split = 0, verbose = 0):\n",
    "        '''\n",
    "        Implementation of stochastic gradient descent with regularization. \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        \n",
    "        if val_split:\n",
    "            ind = int(val_split*len(x_train))\n",
    "            x_val = x_train[:ind]\n",
    "            y_val = y_train[:ind]\n",
    "            x_train = x_train[ind:]\n",
    "            y_train = y_train[ind:]\n",
    "        \n",
    "        #combine the x and ys into one array to make suffling more straightforward\n",
    "        comb = np.c_[x_train.reshape(len(x_train), -1), y_train.reshape(len(y_train), -1)]\n",
    "        x_train_c = comb[:, :x_train.size//len(x_train)].reshape(x_train.shape)\n",
    "        y_train_c = comb[:, x_train.size//len(x_train):].reshape(y_train.shape)\n",
    "        #keep track of total training time and print initial accuracy sans training.\n",
    "        start = time.time()\n",
    "        ita = score = self.score(x_train,y_train)\n",
    "        print(f\"Training on {len(x_train)} samples for {n_iter} epochs on {self.no_layers} layers with {self.no_params} parameters...\")\n",
    "        print(f\"Initial training accuracy: {ita:.4f}%...\")\n",
    "        for epoch in range(1,n_iter+1):\n",
    "            if verbose: print(f\"Running epoch {epoch} of {n_iter}\")\n",
    "            \n",
    "            #This is just a fancy way of shuffling the x and ys together.\n",
    "            x_train_c = comb[:, :x_train.size//len(x_train)].reshape(x_train.shape)\n",
    "            y_train_c = comb[:, x_train.size//len(x_train):].reshape(y_train.shape)\n",
    "\n",
    "            np.random.shuffle(comb)\n",
    "        \n",
    "            for x,y in zip(x_train_c, y_train_c):\n",
    "                self.backward_propagate_error(x,y)\n",
    "                for i,layer in enumerate(self.layers):\n",
    "                    if layer.is_input:\n",
    "                        continue\n",
    "                    else:\n",
    "                        layer.weights = layer.weights - (alpha*np.array(self.delta_w[i])) #.reshape(layer.weights.shape)\n",
    "                        layer.bias = layer.bias - alpha*np.array(self.delta_b[i]) #.reshape(layer.bias.shape)\n",
    "            if val_split:\n",
    "                val_score = self.score(x_val, y_val)\n",
    "            \n",
    "            score = self.score(x_train,y_train)\n",
    "            if verbose: print(f\"Training accuracy: {score:.4f}%\")\n",
    "            if val_split and verbose: print(f\"Validation Accuracy: {val_score:.4f}%\")\n",
    "            #-----------------\n",
    "        end = time.time()  \n",
    "            \n",
    "        score = self.score(x_train,y_train)\n",
    "        val_score = self.score(x_val, y_val)\n",
    "        print(f\"Training accuracy: {score:.4f}%\")\n",
    "        \n",
    "        if val_split: \n",
    "            print(f\"Validation Accuracy: {val_score:.4f}%\")\n",
    "            \n",
    "        print(f\"Ran {n_iter} epochs in {end-start:.2f} seconds\")\n",
    "    \n",
    "    \n",
    "    def score(self, x_test, y_test):\n",
    "        '''\n",
    "        Simple score function that gets the accuracy for a given test set\n",
    "        '''\n",
    "        x_test = np.array(x_test)\n",
    "        y_test = np.array(y_test)\n",
    "        correct = 0\n",
    "        total = len(x_test)\n",
    "        for x,y in zip(x_test, y_test):\n",
    "            self.fwdpropagate(x)\n",
    "            if self.predict(x) ==  y:\n",
    "                correct += 1\n",
    "        return correct/total\n",
    "    \n",
    "    \n",
    "    \n",
    "    def backward_propagate_error(self, x, y):\n",
    "        ''' The backward propogate error function is based on the algorithm from http://cs229.stanford.edu/notes2020spring/cs229-notes-deep_learning.pdf\n",
    "            (on the 2nd last page). Note the difference in derivatives of cost function from these notes, as \n",
    "            the notes use 1/2 squared error as the cost function.\n",
    "            \n",
    "            The function fisrt does the forward propagtaion to ensure the z's and a's of each layer exist. Then,\n",
    "            using the chain rule, we compute the gradients for each weight in each layer.\n",
    "        '''\n",
    "        self.fwdpropagate(x)\n",
    "        delta = [None]*self.no_layers \n",
    "        delta_w = [None]*self.no_layers\n",
    "        delta_b = [None]*self.no_layers\n",
    "   \n",
    "        for i in range(self.no_layers - 1, -1, -1):\n",
    "            if i == self.no_layers - 1:\n",
    "                if y == 1:\n",
    "                    q = (y/(self.layers[i].a+ 10e-7))\n",
    "                else:\n",
    "                    q = -1*((1-y)/(1-(self.layers[i].a- 10e-7)))\n",
    "                delta[i] = -1*q*self.layers[i].act_f.deriv(self.layers[i].z)\n",
    "            else:\n",
    "                delta[i] = ((self.layers[i+1].weights)@(delta[i+1]))*self.layers[i].act_f.deriv(self.layers[i].z).reshape(-1,1)\n",
    "                delta_w[i+1] = ((delta[i+1])@(self.layers[i].a.reshape(1,-1))).T + 2*self.lamb*self.layers[i+1].weights\n",
    "                delta_b[i+1] = delta[i+1]\n",
    "\n",
    "        self.delta_w = delta_w\n",
    "        self.delta_b = delta_b\n",
    "        self.delta = delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network on blobs250 and moons400\n",
    "To create the shallow neural network we must create a list with the number of notes in the input layer, the number in the hidden layer, and the number in the output layer, which will always be 1. It is neccessary to create a list of the same length with the activation function in the respective layers.  \n",
    "  \n",
    "  Note: I do the validation split after the train-test split. So to produce a .70-.15-.15 split I first split .85-.15 train-test on the original data, then a 14/17-3/17 train-validation split on the .85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/oisin/MAI_work/ongoing_assignments/DeepLearning/data/\"\n",
    "\n",
    "data = pd.read_csv(path+\"blobs250.csv\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class'], test_size = 0.15) \n",
    "\n",
    "acts = [IdentityActivation, SigmoidActivation, SigmoidActivation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 175 samples for 10 epochs on 3 layers with 100 parameters...\n",
      "Initial training accuracy: 0.7943%...\n",
      "Training accuracy: 1.0000%\n",
      "Validation Accuracy: 1.0000%\n",
      "Ran 10 epochs in 0.30 seconds\n"
     ]
    }
   ],
   "source": [
    "net = Network([3, 25, 1],activation_function=acts, lamb = 0)\n",
    "net.stochastic_gradient_descent(x_train, y_train, alpha=0.01, n_iter=10, val_split=3/17);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 280 samples for 50 epochs on 3 layers with 75 parameters...\n",
      "Initial training accuracy: 0.4893%...\n",
      "Training accuracy: 0.8643%\n",
      "Validation Accuracy: 0.8667%\n",
      "Ran 50 epochs in 2.18 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path+\"moons400.csv\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class'], test_size=0.15) \n",
    "\n",
    "acts = [IdentityActivation, SigmoidActivation, SigmoidActivation]\n",
    "\n",
    "net = Network([2, 25, 1],activation_function=acts, lamb = 0)\n",
    "\n",
    "net.stochastic_gradient_descent(x_train, y_train, alpha=0.01, n_iter=50, val_split=3/17)\n",
    "\n",
    "net.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "There is naturally no improvement in the easier dataset as the logistic regression had 100% test accracy, as did the shallow net. The shallow net improved on the harder of the two data sets by about 13%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       | Logistic Regression | Shallow Neural Network |\n",
    "|-------|---------------------|------------------------|\n",
    "| Blobs | 100%                | 100%                   |\n",
    "| Moons | 84%                 | 95%                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the cifar-10 image data\n",
    "Below are the functions used to load in the CIFAR-10 data, convert it to greyscale, and split into the training and test sets. Unpickle is supplied by the University of Toronto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-192-7d0abaed5133>:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  _all_data = np.array(data)\n"
     ]
    }
   ],
   "source": [
    "###--- Functions to get data and to modify ---###\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='bytes')\n",
    "    return d\n",
    "\n",
    "def greyscale_data(data):\n",
    "    '''\n",
    "    FORMULA IS L = R * 299/1000 + G * 587/1000 + B * 114/1000\n",
    "    where R,G,B are, obviously the RGB colour values.\n",
    "\n",
    "    In the data we have a vector of length 3*1024. Split this into a matrix of size 3x1024 \n",
    "    then convert using the formula\n",
    "    '''\n",
    "    data_2 = []\n",
    "    for i,x in enumerate(data):\n",
    "        r = x[:1024]\n",
    "        g = x[1024:2048]\n",
    "        b = x[2048:]\n",
    "        grey = ((r * 299/1000) + (g * 587/1000) + (b * 114/1000))/255\n",
    "        data_2.append(grey)\n",
    "    return np.array(data_2)\n",
    "\n",
    "\n",
    "path_to_images = \"/home/oisin/MAI_work/ongoing_assignments/DeepLearning/cifar-10-batches-py/\"\n",
    "# datas = [unpickle(path_to_images+\"data_batch_\"+str(i)) for i in range(1,6)]\n",
    "datas = [unpickle(path_to_images+\"data_batch_1\")]\n",
    "x = [d[b'data'] for d in datas]\n",
    "y = [d[b'labels'] for d in datas]\n",
    "x_ = np.concatenate(x)\n",
    "y_ = np.concatenate(y)\n",
    "data =  [[imput_x, imput_y - 2] for imput_x, imput_y in zip(x_,y_) if imput_y == 2 or imput_y == 3]\n",
    "\n",
    "_all_data = np.array(data)\n",
    "x_data = greyscale_data(_all_data[:,0])\n",
    "y_data = _all_data[:,1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.15)\n",
    "x_train, x_test, y_train, y_test = x_train.astype('f'), x_test.astype('f'), y_train.astype('f'), y_test.astype('f')\n",
    "inp_size = len(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Network on CIFAR-10 data (Cats and Birds)\n",
    "We create a neural network with 1000 nodes in 1 hidden layer, each layer with the sigmoid activation function. The first layer doesn't need an activation function, the IdentityActivation doesn't actually do anything, it is more for illustrative puposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [IdentityActivation, SigmoidActivation, SigmoidActivation]\n",
    "net = Network([x_train.shape[1], 1000, 1],activation_function=acts, lamb = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1433 samples for 100 epochs on 3 layers with 1025000 parameters...\n",
      "Initial training accuracy: 0.5010%...\n",
      "Training accuracy: 0.8618%\n",
      "Validation Accuracy: 0.5505%\n",
      "Ran 100 epochs in 2323.57 seconds\n"
     ]
    }
   ],
   "source": [
    "net.stochastic_gradient_descent(x_train,y_train, alpha=0.005, n_iter=100, val_split=3/17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4902597402597403"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly that the shallow net is not expressive enough to describe the image data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Enhancement\n",
    "As mentioned before, the main enhancement made was the ability to add more hidden layers, but a few extra tweaks were added also. When observing the training of the neural network with one hidden layer there were to major issues that stood out initially. \n",
    "* Firstly, the training was extreamly sensitive to the inital weight values (with respect to the scale I was usuing). A solution is offered by Glorot and Bengio in http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf where they suggest that the weights be distributed as: $$\\mathcal{U}\\left[-\\frac{\\sqrt{6}}{\\sqrt{m+n}},\\frac{\\sqrt{6}}{\\sqrt{m+n}}\\right] $$ \n",
    "where m is the number of inputs to a layer and n is the number of nodes (or outputs) in the layer.\n",
    "\n",
    "* Secondly, the network was extreamly prone to overfitting. To overcome this I introduced $L_2$ regularisation. The implementation came straight from the notes from class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that for a network with 1 hidden layer with 1000 nodes we reach about 62% training accuracy and 54.5% accuracy on the validation set and the test set. This leads us to believe that the network is overfitting on the training data. To combat this we can introduce more layers and a regularisation parameter $\\lambda$. This further adaption allowed the training accuracy to reach 100% and the test accuracy to reach 64%. While still overfitting we have a significant improvement in test accuracy, so the network has generalised somewhat when compared to the single hidden layer without regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [IdentityActivation, ReluActivation,ReluActivation,ReluActivation,ReluActivation,ReluActivation,SigmoidActivation]\n",
    "net = Network([x_train.shape[1],1000,500,500, 250, 100, 1],activation_function=acts, lamb = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1549 samples for 100 epochs on 7 layers with 1924100 parameters...\n",
      "Initial training accuracy: 0.4984%...\n",
      "Training accuracy: 1.0000%\n",
      "Validation Accuracy: 0.6440%\n",
      "Ran 100 epochs in 3954.16 seconds\n"
     ]
    }
   ],
   "source": [
    "net.stochastic_gradient_descent(x_train, y_train, alpha=0.001, n_iter = 100, val_split= 0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6038961038961039"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
